{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i4DGm68Zn7T"
      },
      "source": [
        "# Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVyD1RpdXrcA",
        "outputId": "b62c5fb7-fb83-4d72-afd5-57723287612d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "# This code works best in google colaboratory. Use google colab for this \n",
        "# In this case we will be using Tensor Flow version 2. We will be using the idea of transfer learning on one of the\n",
        "# pre trained models. We will be doing this as an end to end exercise. \n",
        "\n",
        "# We may not need to update the tensorflow \n",
        "# Works best with tf==2.6.0 but 2.8.2 is also a good version \n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRuFaitWX0mw",
        "outputId": "f22902fb-a67a-4711-cde9-95e688d14d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 31.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61 kB 38.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 276 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 327 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 352 kB 31.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (1.2.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (2.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# Lets install some basic libraries \n",
        "!pip install tf_slim\n",
        "!pip install pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOW8CCHLX-g2",
        "outputId": "c55b7263-57fe-4519-b428-ff8afe79ac17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 3491, done.\u001b[K\n",
            "remote: Counting objects: 100% (3491/3491), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2912/2912), done.\u001b[K\n",
            "remote: Total 3491 (delta 909), reused 1422 (delta 526), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3491/3491), 46.95 MiB | 30.94 MiB/s, done.\n",
            "Resolving deltas: 100% (909/909), done.\n"
          ]
        }
      ],
      "source": [
        "# We will be cloning github tensorflow model repository as we want to do some transfer learning with those models\n",
        "# on our current dataset. \n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrOhNjGeYCDF"
      },
      "outputs": [],
      "source": [
        "# Convert proto files into python \n",
        "# Protos conversion\n",
        "%%bash\n",
        "cd /content/models/research\n",
        "protoc object_detection/protos/*.proto --python_out=."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MwsEUREYMH5",
        "outputId": "8f20b8a7-c646-4f8a-8df4-2f2f4ab319c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/models/research/object_detection/packages/tf2\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.41.0-cp37-cp37m-manylinux2010_x86_64.whl (10.9 MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.32)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.10.0-py2.py3-none-any.whl (2.2 MB)\n",
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.0 MB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.8.0)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "Collecting sacrebleu<=2.2.0\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.21.6)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2022.6.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Collecting immutabledict\n",
            "  Downloading immutabledict-2.2.1-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "Collecting tensorflow~=2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "Collecting tensorflow-text~=2.10.0\n",
            "  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "Collecting opencv-python-headless==4.5.2.52\n",
            "  Downloading opencv_python_headless-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.31.6)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2022.2.1)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.56.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Collecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (2.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.48.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.6)\n",
            "Collecting keras\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow~=2.10.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "Requirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
            "Collecting cloudpickle<3,>=2.1.0\n",
            "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting protobuf<4.0.0dev,>=3.12.0\n",
            "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.1.1)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.7.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.9.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.10.0)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Building wheels for collected packages: object-detection, py-cpuinfo, dill, avro-python3, docopt, seqeval\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1236 sha256=1ebf5b54709123c62d68a72b258eaa040c7375c568281a12d25a348bdf0645fc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yszruweh/wheels/d3/af/d9/d9a25faaa2ce4aef17ef6a07f9bb33d386281f8f3580bce973\n",
            "  Building wheel for py-cpuinfo (setup.py): started\n",
            "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=bc12342201f5c24ddc676f10e42999b4ee386b70a83c8281bf8db015a640139b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for dill (setup.py): started\n",
            "  Building wheel for dill (setup.py): finished with status 'done'\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=521da2b2e0363ab73c8ae15a77b8085b40f7c1aa45efda9712d2b303d4fbed3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for avro-python3 (setup.py): started\n",
            "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=1b8cfedfa834642dac586b0ae2aa8fdc712be2052093d3cae473f0990868a2a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=802cc39d0dc4bb9ca7fa69687e561882a2a4f284f3199b84c73451e21141394b\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=938b65d006b337038822b024a197135c9bdb16d4fc34f64b9b1b4160eb1c15ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built object-detection py-cpuinfo dill avro-python3 docopt seqeval\n",
            "Installing collected packages: requests, pyparsing, protobuf, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, keras, gast, tensorflow, portalocker, docopt, dill, colorama, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, proto-plus, orjson, opencv-python-headless, immutabledict, hdfs, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.26.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.26.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.26.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.2.0\n",
            "    Uninstalling pymongo-4.2.0:\n",
            "      Successfully uninstalled pymongo-4.2.0\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.6.0.66\n",
            "    Uninstalling opencv-python-headless-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-headless-4.6.0.66\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "Successfully installed apache-beam-2.41.0 avro-python3-1.10.2 cloudpickle-2.2.0 colorama-0.4.5 dill-0.3.1.1 docopt-0.6.2 fastavro-1.6.1 gast-0.4.0 hdfs-2.7.0 immutabledict-2.2.1 keras-2.10.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.2.52 orjson-3.8.0 portalocker-2.5.1 proto-plus-1.22.1 protobuf-3.19.6 py-cpuinfo-8.0.0 pymongo-3.12.3 pyparsing-2.4.7 pyyaml-5.4.1 requests-2.28.1 sacrebleu-2.2.0 sentencepiece-0.1.97 seqeval-1.2.2 tensorboard-2.10.1 tensorflow-2.10.0 tensorflow-addons-0.18.0 tensorflow-estimator-2.10.0 tensorflow-io-0.27.0 tensorflow-io-gcs-filesystem-0.27.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.10.0 tf-models-official-2.10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n"
          ]
        }
      ],
      "source": [
        "# Lets install setup \n",
        "%%bash \n",
        "cd /content/models/research/object_detection/packages/tf2\n",
        "pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "sdMxS7VVaJDX",
        "outputId": "7e421f9f-1ff0-451b-d67c-2e846469b195"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ade4803c-bdd6-4bd7-b354-c0d92ff11765\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ade4803c-bdd6-4bd7-b354-c0d92ff11765\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "ref                                                            title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "-------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "iamsouravbanerjee/world-population-dataset                     World Population Dataset                             17KB  2022-08-31 11:20:04          12411        358  1.0              \n",
            "whenamancodes/hr-employee-attrition                            Employee Analysis | Attrition Report                 50KB  2022-09-12 10:46:33           1037         30  1.0              \n",
            "pantanjali/unemployment-dataset                                Unemployment dataset                                 17KB  2022-09-08 08:26:10           4201        105  1.0              \n",
            "whenamancodes/student-performance                              Student Performance                                  18KB  2022-09-15 01:15:37           2031         48  1.0              \n",
            "whenamancodes/students-performance-in-exams                    Students Performance in Exams                         9KB  2022-09-14 15:14:54           2189         48  1.0              \n",
            "harshsingh2209/tesla-stock-pricing-20172022                    TESLA stock pricing (2017-2022)                      28KB  2022-09-18 14:56:29            931         29  1.0              \n",
            "thedevastator/airplane-crashes-and-fatalities                  Airplane Crashes and Fatalities                     582KB  2022-09-20 05:30:35           1352         52  0.9411765        \n",
            "ariyoomotade/netflix-data-cleaning-analysis-and-visualization  Netflix Data: Cleaning, Analysis and Visualization  270KB  2022-08-26 09:25:43           6330        146  1.0              \n",
            "alexandrepetit881234/korean-demographics-20002022              Korean demographics 2000-2022                       101KB  2022-09-15 11:59:31            508         28  1.0              \n",
            "whenamancodes/netflix-prime-video-disney-hulu                  Netflix Disney+ Prime Video Hulu Shows Collection   101KB  2022-09-13 09:05:20           1119         30  1.0              \n",
            "whenamancodes/alcohol-effects-on-study                         Alcohol Effects On Study                             18KB  2022-09-15 03:21:04           1374         51  1.0              \n",
            "thedevastator/mcdonalds-ice-cream-machines-broken-timeseries   McDonalds Ice Cream Machines Breaking - Timeseries  404KB  2022-09-14 23:51:09           1257         42  1.0              \n",
            "whenamancodes/violence-against-women-girls                     Violence Against Women & Girls                       88KB  2022-09-12 08:46:49            999         42  1.0              \n",
            "deepcontractor/smoke-detection-dataset                         Smoke Detection Dataset                               2MB  2022-08-21 06:29:34           3843        105  1.0              \n",
            "thedevastator/weather-prediction                               Weather Prediction                                  936KB  2022-09-06 12:07:29            873         32  0.9705882        \n",
            "cashncarry/fifa-23-complete-player-dataset                     FIFA 23 Complete Player Dataset [UPD:29/09/22]        2MB  2022-09-29 19:34:27            601         29  0.88235295       \n",
            "whenamancodes/data-science-fields-salary-categorization        Data Science Fields Salary Categorization             7KB  2022-09-10 07:53:45           1265         43  1.0              \n",
            "moazzimalibhatti/co2-emission-by-countries-year-wise-17502022  CO2 Emission by countries Year wise (1750-2022)     280KB  2022-09-14 07:43:00           1057         55  1.0              \n",
            "sergylog/ab-test-data                                          A/B test data                                        28KB  2022-09-16 17:29:06            618         97  1.0              \n",
            "advaypatil/youtube-statistics                                  Youtube Statistics                                    2MB  2022-08-26 02:03:19           2810         75  1.0              \n"
          ]
        }
      ],
      "source": [
        "# Download Kaggle datasets \n",
        "# Follow this : kaggle.com/general/74235\n",
        "#kaggle datasets download -d sshikamaru/car-object-detection\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp \"/content/kaggle.json\" ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJz0yJmoYkAb",
        "outputId": "6e4247d0-f2f3-495a-cb62-9d7cdaa1b0fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading car-object-detection.zip to /content\n",
            " 96% 108M/112M [00:00<00:00, 135MB/s]  \n",
            "100% 112M/112M [00:00<00:00, 119MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d sshikamaru/car-object-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiIVdJlUcxKm",
        "outputId": "64e9bf35-02d4-44ae-c582-0b7c075f8088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/car-object-detection.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "  inflating: data/testing_images/vid_5_25100.jpg  \n",
            "  inflating: data/testing_images/vid_5_25120.jpg  \n",
            "  inflating: data/testing_images/vid_5_25140.jpg  \n",
            "  inflating: data/testing_images/vid_5_25160.jpg  \n",
            "  inflating: data/testing_images/vid_5_25180.jpg  \n",
            "  inflating: data/testing_images/vid_5_25200.jpg  \n",
            "  inflating: data/testing_images/vid_5_25220.jpg  \n",
            "  inflating: data/testing_images/vid_5_25240.jpg  \n",
            "  inflating: data/testing_images/vid_5_25260.jpg  \n",
            "  inflating: data/testing_images/vid_5_26320.jpg  \n",
            "  inflating: data/testing_images/vid_5_26400.jpg  \n",
            "  inflating: data/testing_images/vid_5_26420.jpg  \n",
            "  inflating: data/testing_images/vid_5_26560.jpg  \n",
            "  inflating: data/testing_images/vid_5_26580.jpg  \n",
            "  inflating: data/testing_images/vid_5_26600.jpg  \n",
            "  inflating: data/testing_images/vid_5_26620.jpg  \n",
            "  inflating: data/testing_images/vid_5_26640.jpg  \n",
            "  inflating: data/testing_images/vid_5_26660.jpg  \n",
            "  inflating: data/testing_images/vid_5_26680.jpg  \n",
            "  inflating: data/testing_images/vid_5_26700.jpg  \n",
            "  inflating: data/testing_images/vid_5_26720.jpg  \n",
            "  inflating: data/testing_images/vid_5_26740.jpg  \n",
            "  inflating: data/testing_images/vid_5_26760.jpg  \n",
            "  inflating: data/testing_images/vid_5_26780.jpg  \n",
            "  inflating: data/testing_images/vid_5_26800.jpg  \n",
            "  inflating: data/testing_images/vid_5_26820.jpg  \n",
            "  inflating: data/testing_images/vid_5_26840.jpg  \n",
            "  inflating: data/testing_images/vid_5_26860.jpg  \n",
            "  inflating: data/testing_images/vid_5_26880.jpg  \n",
            "  inflating: data/testing_images/vid_5_26900.jpg  \n",
            "  inflating: data/testing_images/vid_5_26920.jpg  \n",
            "  inflating: data/testing_images/vid_5_26940.jpg  \n",
            "  inflating: data/testing_images/vid_5_26960.jpg  \n",
            "  inflating: data/testing_images/vid_5_26980.jpg  \n",
            "  inflating: data/testing_images/vid_5_27240.jpg  \n",
            "  inflating: data/testing_images/vid_5_27260.jpg  \n",
            "  inflating: data/testing_images/vid_5_27280.jpg  \n",
            "  inflating: data/testing_images/vid_5_27300.jpg  \n",
            "  inflating: data/testing_images/vid_5_27320.jpg  \n",
            "  inflating: data/testing_images/vid_5_27360.jpg  \n",
            "  inflating: data/testing_images/vid_5_27380.jpg  \n",
            "  inflating: data/testing_images/vid_5_27400.jpg  \n",
            "  inflating: data/testing_images/vid_5_27420.jpg  \n",
            "  inflating: data/testing_images/vid_5_27440.jpg  \n",
            "  inflating: data/testing_images/vid_5_27460.jpg  \n",
            "  inflating: data/testing_images/vid_5_27480.jpg  \n",
            "  inflating: data/testing_images/vid_5_27500.jpg  \n",
            "  inflating: data/testing_images/vid_5_27520.jpg  \n",
            "  inflating: data/testing_images/vid_5_27540.jpg  \n",
            "  inflating: data/testing_images/vid_5_27560.jpg  \n",
            "  inflating: data/testing_images/vid_5_27580.jpg  \n",
            "  inflating: data/testing_images/vid_5_27600.jpg  \n",
            "  inflating: data/testing_images/vid_5_27620.jpg  \n",
            "  inflating: data/testing_images/vid_5_27640.jpg  \n",
            "  inflating: data/testing_images/vid_5_27660.jpg  \n",
            "  inflating: data/testing_images/vid_5_27680.jpg  \n",
            "  inflating: data/testing_images/vid_5_27700.jpg  \n",
            "  inflating: data/testing_images/vid_5_27720.jpg  \n",
            "  inflating: data/testing_images/vid_5_27740.jpg  \n",
            "  inflating: data/testing_images/vid_5_27760.jpg  \n",
            "  inflating: data/testing_images/vid_5_27780.jpg  \n",
            "  inflating: data/testing_images/vid_5_27800.jpg  \n",
            "  inflating: data/testing_images/vid_5_27820.jpg  \n",
            "  inflating: data/testing_images/vid_5_27840.jpg  \n",
            "  inflating: data/testing_images/vid_5_27860.jpg  \n",
            "  inflating: data/testing_images/vid_5_27880.jpg  \n",
            "  inflating: data/testing_images/vid_5_27900.jpg  \n",
            "  inflating: data/testing_images/vid_5_27920.jpg  \n",
            "  inflating: data/testing_images/vid_5_27940.jpg  \n",
            "  inflating: data/testing_images/vid_5_27960.jpg  \n",
            "  inflating: data/testing_images/vid_5_27980.jpg  \n",
            "  inflating: data/testing_images/vid_5_28000.jpg  \n",
            "  inflating: data/testing_images/vid_5_28020.jpg  \n",
            "  inflating: data/testing_images/vid_5_28040.jpg  \n",
            "  inflating: data/testing_images/vid_5_28060.jpg  \n",
            "  inflating: data/testing_images/vid_5_28080.jpg  \n",
            "  inflating: data/testing_images/vid_5_28180.jpg  \n",
            "  inflating: data/testing_images/vid_5_28260.jpg  \n",
            "  inflating: data/testing_images/vid_5_28320.jpg  \n",
            "  inflating: data/testing_images/vid_5_28340.jpg  \n",
            "  inflating: data/testing_images/vid_5_28360.jpg  \n",
            "  inflating: data/testing_images/vid_5_28380.jpg  \n",
            "  inflating: data/testing_images/vid_5_28420.jpg  \n",
            "  inflating: data/testing_images/vid_5_28440.jpg  \n",
            "  inflating: data/testing_images/vid_5_28460.jpg  \n",
            "  inflating: data/testing_images/vid_5_28480.jpg  \n",
            "  inflating: data/testing_images/vid_5_28500.jpg  \n",
            "  inflating: data/testing_images/vid_5_28520.jpg  \n",
            "  inflating: data/testing_images/vid_5_28540.jpg  \n",
            "  inflating: data/testing_images/vid_5_28560.jpg  \n",
            "  inflating: data/testing_images/vid_5_28580.jpg  \n",
            "  inflating: data/testing_images/vid_5_28600.jpg  \n",
            "  inflating: data/testing_images/vid_5_28620.jpg  \n",
            "  inflating: data/testing_images/vid_5_28640.jpg  \n",
            "  inflating: data/testing_images/vid_5_28660.jpg  \n",
            "  inflating: data/testing_images/vid_5_28680.jpg  \n",
            "  inflating: data/testing_images/vid_5_28700.jpg  \n",
            "  inflating: data/testing_images/vid_5_29000.jpg  \n",
            "  inflating: data/testing_images/vid_5_29020.jpg  \n",
            "  inflating: data/testing_images/vid_5_29040.jpg  \n",
            "  inflating: data/testing_images/vid_5_29060.jpg  \n",
            "  inflating: data/testing_images/vid_5_29080.jpg  \n",
            "  inflating: data/testing_images/vid_5_29100.jpg  \n",
            "  inflating: data/testing_images/vid_5_29400.jpg  \n",
            "  inflating: data/testing_images/vid_5_29420.jpg  \n",
            "  inflating: data/testing_images/vid_5_29440.jpg  \n",
            "  inflating: data/testing_images/vid_5_29460.jpg  \n",
            "  inflating: data/testing_images/vid_5_29480.jpg  \n",
            "  inflating: data/testing_images/vid_5_29500.jpg  \n",
            "  inflating: data/testing_images/vid_5_29520.jpg  \n",
            "  inflating: data/testing_images/vid_5_29540.jpg  \n",
            "  inflating: data/testing_images/vid_5_29560.jpg  \n",
            "  inflating: data/testing_images/vid_5_29580.jpg  \n",
            "  inflating: data/testing_images/vid_5_29600.jpg  \n",
            "  inflating: data/testing_images/vid_5_29620.jpg  \n",
            "  inflating: data/testing_images/vid_5_29640.jpg  \n",
            "  inflating: data/testing_images/vid_5_29720.jpg  \n",
            "  inflating: data/testing_images/vid_5_29740.jpg  \n",
            "  inflating: data/testing_images/vid_5_29760.jpg  \n",
            "  inflating: data/testing_images/vid_5_29820.jpg  \n",
            "  inflating: data/testing_images/vid_5_29840.jpg  \n",
            "  inflating: data/testing_images/vid_5_29860.jpg  \n",
            "  inflating: data/testing_images/vid_5_29880.jpg  \n",
            "  inflating: data/testing_images/vid_5_29900.jpg  \n",
            "  inflating: data/testing_images/vid_5_29980.jpg  \n",
            "  inflating: data/testing_images/vid_5_30000.jpg  \n",
            "  inflating: data/testing_images/vid_5_30020.jpg  \n",
            "  inflating: data/testing_images/vid_5_30040.jpg  \n",
            "  inflating: data/testing_images/vid_5_30120.jpg  \n",
            "  inflating: data/testing_images/vid_5_30140.jpg  \n",
            "  inflating: data/testing_images/vid_5_30160.jpg  \n",
            "  inflating: data/testing_images/vid_5_30180.jpg  \n",
            "  inflating: data/testing_images/vid_5_30640.jpg  \n",
            "  inflating: data/testing_images/vid_5_30660.jpg  \n",
            "  inflating: data/testing_images/vid_5_30680.jpg  \n",
            "  inflating: data/testing_images/vid_5_30700.jpg  \n",
            "  inflating: data/testing_images/vid_5_30720.jpg  \n",
            "  inflating: data/testing_images/vid_5_30740.jpg  \n",
            "  inflating: data/testing_images/vid_5_30760.jpg  \n",
            "  inflating: data/testing_images/vid_5_30820.jpg  \n",
            "  inflating: data/testing_images/vid_5_30840.jpg  \n",
            "  inflating: data/testing_images/vid_5_30860.jpg  \n",
            "  inflating: data/testing_images/vid_5_30920.jpg  \n",
            "  inflating: data/testing_images/vid_5_30940.jpg  \n",
            "  inflating: data/testing_images/vid_5_31020.jpg  \n",
            "  inflating: data/testing_images/vid_5_31040.jpg  \n",
            "  inflating: data/testing_images/vid_5_31060.jpg  \n",
            "  inflating: data/testing_images/vid_5_31080.jpg  \n",
            "  inflating: data/testing_images/vid_5_31100.jpg  \n",
            "  inflating: data/testing_images/vid_5_31120.jpg  \n",
            "  inflating: data/testing_images/vid_5_31140.jpg  \n",
            "  inflating: data/testing_images/vid_5_31160.jpg  \n",
            "  inflating: data/testing_images/vid_5_31180.jpg  \n",
            "  inflating: data/testing_images/vid_5_31200.jpg  \n",
            "  inflating: data/testing_images/vid_5_31260.jpg  \n",
            "  inflating: data/testing_images/vid_5_31280.jpg  \n",
            "  inflating: data/testing_images/vid_5_31300.jpg  \n",
            "  inflating: data/testing_images/vid_5_31360.jpg  \n",
            "  inflating: data/testing_images/vid_5_31380.jpg  \n",
            "  inflating: data/testing_images/vid_5_31400.jpg  \n",
            "  inflating: data/testing_images/vid_5_31420.jpg  \n",
            "  inflating: data/testing_images/vid_5_31480.jpg  \n",
            "  inflating: data/testing_images/vid_5_31500.jpg  \n",
            "  inflating: data/testing_images/vid_5_31520.jpg  \n",
            "  inflating: data/testing_images/vid_5_31560.jpg  \n",
            "  inflating: data/testing_images/vid_5_31600.jpg  \n",
            "  inflating: data/testing_images/vid_5_31620.jpg  \n",
            "  inflating: data/testing_images/vid_5_31640.jpg  \n",
            "  inflating: data/testing_images/vid_5_31660.jpg  \n",
            "  inflating: data/testing_images/vid_5_31680.jpg  \n",
            "  inflating: data/testing_images/vid_5_31700.jpg  \n",
            "  inflating: data/testing_images/vid_5_31720.jpg  \n",
            "  inflating: data/testing_images/vid_5_400.jpg  \n",
            "  inflating: data/testing_images/vid_5_420.jpg  \n",
            "  inflating: data/testing_images/vid_5_440.jpg  \n",
            "  inflating: data/train_solution_bounding_boxes (1).csv  \n",
            "  inflating: data/training_images/vid_4_1000.jpg  \n",
            "  inflating: data/training_images/vid_4_10000.jpg  \n",
            "  inflating: data/training_images/vid_4_10020.jpg  \n",
            "  inflating: data/training_images/vid_4_10040.jpg  \n",
            "  inflating: data/training_images/vid_4_10060.jpg  \n",
            "  inflating: data/training_images/vid_4_10080.jpg  \n",
            "  inflating: data/training_images/vid_4_10100.jpg  \n",
            "  inflating: data/training_images/vid_4_10120.jpg  \n",
            "  inflating: data/training_images/vid_4_10140.jpg  \n",
            "  inflating: data/training_images/vid_4_10160.jpg  \n",
            "  inflating: data/training_images/vid_4_10180.jpg  \n",
            "  inflating: data/training_images/vid_4_1020.jpg  \n",
            "  inflating: data/training_images/vid_4_10200.jpg  \n",
            "  inflating: data/training_images/vid_4_10220.jpg  \n",
            "  inflating: data/training_images/vid_4_10240.jpg  \n",
            "  inflating: data/training_images/vid_4_10260.jpg  \n",
            "  inflating: data/training_images/vid_4_10280.jpg  \n",
            "  inflating: data/training_images/vid_4_10300.jpg  \n",
            "  inflating: data/training_images/vid_4_10320.jpg  \n",
            "  inflating: data/training_images/vid_4_10340.jpg  \n",
            "  inflating: data/training_images/vid_4_10360.jpg  \n",
            "  inflating: data/training_images/vid_4_10380.jpg  \n",
            "  inflating: data/training_images/vid_4_1040.jpg  \n",
            "  inflating: data/training_images/vid_4_10400.jpg  \n",
            "  inflating: data/training_images/vid_4_10420.jpg  \n",
            "  inflating: data/training_images/vid_4_10440.jpg  \n",
            "  inflating: data/training_images/vid_4_10460.jpg  \n",
            "  inflating: data/training_images/vid_4_10480.jpg  \n",
            "  inflating: data/training_images/vid_4_10500.jpg  \n",
            "  inflating: data/training_images/vid_4_10520.jpg  \n",
            "  inflating: data/training_images/vid_4_10540.jpg  \n",
            "  inflating: data/training_images/vid_4_10560.jpg  \n",
            "  inflating: data/training_images/vid_4_10580.jpg  \n",
            "  inflating: data/training_images/vid_4_1060.jpg  \n",
            "  inflating: data/training_images/vid_4_10600.jpg  \n",
            "  inflating: data/training_images/vid_4_10620.jpg  \n",
            "  inflating: data/training_images/vid_4_10640.jpg  \n",
            "  inflating: data/training_images/vid_4_10660.jpg  \n",
            "  inflating: data/training_images/vid_4_10680.jpg  \n",
            "  inflating: data/training_images/vid_4_10700.jpg  \n",
            "  inflating: data/training_images/vid_4_10720.jpg  \n",
            "  inflating: data/training_images/vid_4_10740.jpg  \n",
            "  inflating: data/training_images/vid_4_10760.jpg  \n",
            "  inflating: data/training_images/vid_4_10780.jpg  \n",
            "  inflating: data/training_images/vid_4_1080.jpg  \n",
            "  inflating: data/training_images/vid_4_10800.jpg  \n",
            "  inflating: data/training_images/vid_4_10820.jpg  \n",
            "  inflating: data/training_images/vid_4_10840.jpg  \n",
            "  inflating: data/training_images/vid_4_10860.jpg  \n",
            "  inflating: data/training_images/vid_4_10880.jpg  \n",
            "  inflating: data/training_images/vid_4_10900.jpg  \n",
            "  inflating: data/training_images/vid_4_10920.jpg  \n",
            "  inflating: data/training_images/vid_4_10940.jpg  \n",
            "  inflating: data/training_images/vid_4_10960.jpg  \n",
            "  inflating: data/training_images/vid_4_10980.jpg  \n",
            "  inflating: data/training_images/vid_4_11000.jpg  \n",
            "  inflating: data/training_images/vid_4_11020.jpg  \n",
            "  inflating: data/training_images/vid_4_11040.jpg  \n",
            "  inflating: data/training_images/vid_4_11060.jpg  \n",
            "  inflating: data/training_images/vid_4_11080.jpg  \n",
            "  inflating: data/training_images/vid_4_11100.jpg  \n",
            "  inflating: data/training_images/vid_4_11120.jpg  \n",
            "  inflating: data/training_images/vid_4_11140.jpg  \n",
            "  inflating: data/training_images/vid_4_11160.jpg  \n",
            "  inflating: data/training_images/vid_4_11180.jpg  \n",
            "  inflating: data/training_images/vid_4_11200.jpg  \n",
            "  inflating: data/training_images/vid_4_11220.jpg  \n",
            "  inflating: data/training_images/vid_4_11240.jpg  \n",
            "  inflating: data/training_images/vid_4_11260.jpg  \n",
            "  inflating: data/training_images/vid_4_11280.jpg  \n",
            "  inflating: data/training_images/vid_4_11300.jpg  \n",
            "  inflating: data/training_images/vid_4_11320.jpg  \n",
            "  inflating: data/training_images/vid_4_11340.jpg  \n",
            "  inflating: data/training_images/vid_4_11360.jpg  \n",
            "  inflating: data/training_images/vid_4_11380.jpg  \n",
            "  inflating: data/training_images/vid_4_11400.jpg  \n",
            "  inflating: data/training_images/vid_4_11420.jpg  \n",
            "  inflating: data/training_images/vid_4_11440.jpg  \n",
            "  inflating: data/training_images/vid_4_11460.jpg  \n",
            "  inflating: data/training_images/vid_4_11480.jpg  \n",
            "  inflating: data/training_images/vid_4_11500.jpg  \n",
            "  inflating: data/training_images/vid_4_11520.jpg  \n",
            "  inflating: data/training_images/vid_4_11760.jpg  \n",
            "  inflating: data/training_images/vid_4_11780.jpg  \n",
            "  inflating: data/training_images/vid_4_11800.jpg  \n",
            "  inflating: data/training_images/vid_4_11820.jpg  \n",
            "  inflating: data/training_images/vid_4_11840.jpg  \n",
            "  inflating: data/training_images/vid_4_11860.jpg  \n",
            "  inflating: data/training_images/vid_4_11880.jpg  \n",
            "  inflating: data/training_images/vid_4_11900.jpg  \n",
            "  inflating: data/training_images/vid_4_11920.jpg  \n",
            "  inflating: data/training_images/vid_4_11940.jpg  \n",
            "  inflating: data/training_images/vid_4_11960.jpg  \n",
            "  inflating: data/training_images/vid_4_11980.jpg  \n",
            "  inflating: data/training_images/vid_4_12000.jpg  \n",
            "  inflating: data/training_images/vid_4_12020.jpg  \n",
            "  inflating: data/training_images/vid_4_12040.jpg  \n",
            "  inflating: data/training_images/vid_4_12060.jpg  \n",
            "  inflating: data/training_images/vid_4_12080.jpg  \n",
            "  inflating: data/training_images/vid_4_12100.jpg  \n",
            "  inflating: data/training_images/vid_4_12120.jpg  \n",
            "  inflating: data/training_images/vid_4_12140.jpg  \n",
            "  inflating: data/training_images/vid_4_12160.jpg  \n",
            "  inflating: data/training_images/vid_4_12180.jpg  \n",
            "  inflating: data/training_images/vid_4_12200.jpg  \n",
            "  inflating: data/training_images/vid_4_12220.jpg  \n",
            "  inflating: data/training_images/vid_4_12240.jpg  \n",
            "  inflating: data/training_images/vid_4_12260.jpg  \n",
            "  inflating: data/training_images/vid_4_12280.jpg  \n",
            "  inflating: data/training_images/vid_4_12300.jpg  \n",
            "  inflating: data/training_images/vid_4_12320.jpg  \n",
            "  inflating: data/training_images/vid_4_12340.jpg  \n",
            "  inflating: data/training_images/vid_4_12360.jpg  \n",
            "  inflating: data/training_images/vid_4_12380.jpg  \n",
            "  inflating: data/training_images/vid_4_12400.jpg  \n",
            "  inflating: data/training_images/vid_4_12420.jpg  \n",
            "  inflating: data/training_images/vid_4_12440.jpg  \n",
            "  inflating: data/training_images/vid_4_12460.jpg  \n",
            "  inflating: data/training_images/vid_4_12480.jpg  \n",
            "  inflating: data/training_images/vid_4_12500.jpg  \n",
            "  inflating: data/training_images/vid_4_12520.jpg  \n",
            "  inflating: data/training_images/vid_4_12540.jpg  \n",
            "  inflating: data/training_images/vid_4_12560.jpg  \n",
            "  inflating: data/training_images/vid_4_12580.jpg  \n",
            "  inflating: data/training_images/vid_4_12600.jpg  \n",
            "  inflating: data/training_images/vid_4_12620.jpg  \n",
            "  inflating: data/training_images/vid_4_12640.jpg  \n",
            "  inflating: data/training_images/vid_4_12660.jpg  \n",
            "  inflating: data/training_images/vid_4_12680.jpg  \n",
            "  inflating: data/training_images/vid_4_12700.jpg  \n",
            "  inflating: data/training_images/vid_4_12720.jpg  \n",
            "  inflating: data/training_images/vid_4_12740.jpg  \n",
            "  inflating: data/training_images/vid_4_12760.jpg  \n",
            "  inflating: data/training_images/vid_4_12780.jpg  \n",
            "  inflating: data/training_images/vid_4_12800.jpg  \n",
            "  inflating: data/training_images/vid_4_12820.jpg  \n",
            "  inflating: data/training_images/vid_4_12840.jpg  \n",
            "  inflating: data/training_images/vid_4_12860.jpg  \n",
            "  inflating: data/training_images/vid_4_12880.jpg  \n",
            "  inflating: data/training_images/vid_4_12900.jpg  \n",
            "  inflating: data/training_images/vid_4_13020.jpg  \n",
            "  inflating: data/training_images/vid_4_13040.jpg  \n",
            "  inflating: data/training_images/vid_4_13060.jpg  \n",
            "  inflating: data/training_images/vid_4_13080.jpg  \n",
            "  inflating: data/training_images/vid_4_13100.jpg  \n",
            "  inflating: data/training_images/vid_4_13120.jpg  \n",
            "  inflating: data/training_images/vid_4_13240.jpg  \n",
            "  inflating: data/training_images/vid_4_13260.jpg  \n",
            "  inflating: data/training_images/vid_4_13280.jpg  \n",
            "  inflating: data/training_images/vid_4_13300.jpg  \n",
            "  inflating: data/training_images/vid_4_13320.jpg  \n",
            "  inflating: data/training_images/vid_4_13420.jpg  \n",
            "  inflating: data/training_images/vid_4_13440.jpg  \n",
            "  inflating: data/training_images/vid_4_13460.jpg  \n",
            "  inflating: data/training_images/vid_4_13480.jpg  \n",
            "  inflating: data/training_images/vid_4_13540.jpg  \n",
            "  inflating: data/training_images/vid_4_13580.jpg  \n",
            "  inflating: data/training_images/vid_4_13620.jpg  \n",
            "  inflating: data/training_images/vid_4_13640.jpg  \n",
            "  inflating: data/training_images/vid_4_13660.jpg  \n",
            "  inflating: data/training_images/vid_4_13680.jpg  \n",
            "  inflating: data/training_images/vid_4_13700.jpg  \n",
            "  inflating: data/training_images/vid_4_13720.jpg  \n",
            "  inflating: data/training_images/vid_4_13740.jpg  \n",
            "  inflating: data/training_images/vid_4_13760.jpg  \n",
            "  inflating: data/training_images/vid_4_13780.jpg  \n",
            "  inflating: data/training_images/vid_4_13800.jpg  \n",
            "  inflating: data/training_images/vid_4_13820.jpg  \n",
            "  inflating: data/training_images/vid_4_13840.jpg  \n",
            "  inflating: data/training_images/vid_4_13860.jpg  \n",
            "  inflating: data/training_images/vid_4_13880.jpg  \n",
            "  inflating: data/training_images/vid_4_13900.jpg  \n",
            "  inflating: data/training_images/vid_4_13920.jpg  \n",
            "  inflating: data/training_images/vid_4_13940.jpg  \n",
            "  inflating: data/training_images/vid_4_13960.jpg  \n",
            "  inflating: data/training_images/vid_4_13980.jpg  \n",
            "  inflating: data/training_images/vid_4_14040.jpg  \n",
            "  inflating: data/training_images/vid_4_14060.jpg  \n",
            "  inflating: data/training_images/vid_4_14080.jpg  \n",
            "  inflating: data/training_images/vid_4_14100.jpg  \n",
            "  inflating: data/training_images/vid_4_14120.jpg  \n",
            "  inflating: data/training_images/vid_4_14140.jpg  \n",
            "  inflating: data/training_images/vid_4_14160.jpg  \n",
            "  inflating: data/training_images/vid_4_14180.jpg  \n",
            "  inflating: data/training_images/vid_4_14200.jpg  \n",
            "  inflating: data/training_images/vid_4_14220.jpg  \n",
            "  inflating: data/training_images/vid_4_14240.jpg  \n",
            "  inflating: data/training_images/vid_4_14260.jpg  \n",
            "  inflating: data/training_images/vid_4_14280.jpg  \n",
            "  inflating: data/training_images/vid_4_14300.jpg  \n",
            "  inflating: data/training_images/vid_4_14320.jpg  \n",
            "  inflating: data/training_images/vid_4_14340.jpg  \n",
            "  inflating: data/training_images/vid_4_14360.jpg  \n",
            "  inflating: data/training_images/vid_4_14380.jpg  \n",
            "  inflating: data/training_images/vid_4_14400.jpg  \n",
            "  inflating: data/training_images/vid_4_14420.jpg  \n",
            "  inflating: data/training_images/vid_4_14440.jpg  \n",
            "  inflating: data/training_images/vid_4_14460.jpg  \n",
            "  inflating: data/training_images/vid_4_14480.jpg  \n",
            "  inflating: data/training_images/vid_4_14500.jpg  \n",
            "  inflating: data/training_images/vid_4_14520.jpg  \n",
            "  inflating: data/training_images/vid_4_14540.jpg  \n",
            "  inflating: data/training_images/vid_4_14560.jpg  \n",
            "  inflating: data/training_images/vid_4_14580.jpg  \n",
            "  inflating: data/training_images/vid_4_14600.jpg  \n",
            "  inflating: data/training_images/vid_4_14620.jpg  \n",
            "  inflating: data/training_images/vid_4_14640.jpg  \n",
            "  inflating: data/training_images/vid_4_14660.jpg  \n",
            "  inflating: data/training_images/vid_4_14680.jpg  \n",
            "  inflating: data/training_images/vid_4_14700.jpg  \n",
            "  inflating: data/training_images/vid_4_14720.jpg  \n",
            "  inflating: data/training_images/vid_4_14740.jpg  \n",
            "  inflating: data/training_images/vid_4_14760.jpg  \n",
            "  inflating: data/training_images/vid_4_14780.jpg  \n",
            "  inflating: data/training_images/vid_4_14800.jpg  \n",
            "  inflating: data/training_images/vid_4_14820.jpg  \n",
            "  inflating: data/training_images/vid_4_14840.jpg  \n",
            "  inflating: data/training_images/vid_4_14860.jpg  \n",
            "  inflating: data/training_images/vid_4_14880.jpg  \n",
            "  inflating: data/training_images/vid_4_14900.jpg  \n",
            "  inflating: data/training_images/vid_4_14920.jpg  \n",
            "  inflating: data/training_images/vid_4_14940.jpg  \n",
            "  inflating: data/training_images/vid_4_14960.jpg  \n",
            "  inflating: data/training_images/vid_4_14980.jpg  \n",
            "  inflating: data/training_images/vid_4_15000.jpg  \n",
            "  inflating: data/training_images/vid_4_15020.jpg  \n",
            "  inflating: data/training_images/vid_4_15040.jpg  \n",
            "  inflating: data/training_images/vid_4_15060.jpg  \n",
            "  inflating: data/training_images/vid_4_15080.jpg  \n",
            "  inflating: data/training_images/vid_4_15100.jpg  \n",
            "  inflating: data/training_images/vid_4_15120.jpg  \n",
            "  inflating: data/training_images/vid_4_15140.jpg  \n",
            "  inflating: data/training_images/vid_4_15160.jpg  \n",
            "  inflating: data/training_images/vid_4_15180.jpg  \n",
            "  inflating: data/training_images/vid_4_1520.jpg  \n",
            "  inflating: data/training_images/vid_4_15200.jpg  \n",
            "  inflating: data/training_images/vid_4_15220.jpg  \n",
            "  inflating: data/training_images/vid_4_1540.jpg  \n",
            "  inflating: data/training_images/vid_4_1560.jpg  \n",
            "  inflating: data/training_images/vid_4_1580.jpg  \n",
            "  inflating: data/training_images/vid_4_1600.jpg  \n",
            "  inflating: data/training_images/vid_4_16000.jpg  \n",
            "  inflating: data/training_images/vid_4_16020.jpg  \n",
            "  inflating: data/training_images/vid_4_16040.jpg  \n",
            "  inflating: data/training_images/vid_4_16060.jpg  \n",
            "  inflating: data/training_images/vid_4_16080.jpg  \n",
            "  inflating: data/training_images/vid_4_16100.jpg  \n",
            "  inflating: data/training_images/vid_4_16120.jpg  \n",
            "  inflating: data/training_images/vid_4_16140.jpg  \n",
            "  inflating: data/training_images/vid_4_16160.jpg  \n",
            "  inflating: data/training_images/vid_4_16180.jpg  \n",
            "  inflating: data/training_images/vid_4_1620.jpg  \n",
            "  inflating: data/training_images/vid_4_16200.jpg  \n",
            "  inflating: data/training_images/vid_4_16220.jpg  \n",
            "  inflating: data/training_images/vid_4_16240.jpg  \n",
            "  inflating: data/training_images/vid_4_16260.jpg  \n",
            "  inflating: data/training_images/vid_4_16280.jpg  \n",
            "  inflating: data/training_images/vid_4_16300.jpg  \n",
            "  inflating: data/training_images/vid_4_16320.jpg  \n",
            "  inflating: data/training_images/vid_4_1640.jpg  \n",
            "  inflating: data/training_images/vid_4_16400.jpg  \n",
            "  inflating: data/training_images/vid_4_16420.jpg  \n",
            "  inflating: data/training_images/vid_4_16440.jpg  \n",
            "  inflating: data/training_images/vid_4_16460.jpg  \n",
            "  inflating: data/training_images/vid_4_16480.jpg  \n",
            "  inflating: data/training_images/vid_4_16500.jpg  \n",
            "  inflating: data/training_images/vid_4_16520.jpg  \n",
            "  inflating: data/training_images/vid_4_16540.jpg  \n",
            "  inflating: data/training_images/vid_4_16560.jpg  \n",
            "  inflating: data/training_images/vid_4_16580.jpg  \n",
            "  inflating: data/training_images/vid_4_1660.jpg  \n",
            "  inflating: data/training_images/vid_4_16600.jpg  \n",
            "  inflating: data/training_images/vid_4_16620.jpg  \n",
            "  inflating: data/training_images/vid_4_16640.jpg  \n",
            "  inflating: data/training_images/vid_4_16660.jpg  \n",
            "  inflating: data/training_images/vid_4_16680.jpg  \n",
            "  inflating: data/training_images/vid_4_16700.jpg  \n",
            "  inflating: data/training_images/vid_4_16720.jpg  \n",
            "  inflating: data/training_images/vid_4_16740.jpg  \n",
            "  inflating: data/training_images/vid_4_16760.jpg  \n",
            "  inflating: data/training_images/vid_4_16780.jpg  \n",
            "  inflating: data/training_images/vid_4_16800.jpg  \n",
            "  inflating: data/training_images/vid_4_16820.jpg  \n",
            "  inflating: data/training_images/vid_4_16900.jpg  \n",
            "  inflating: data/training_images/vid_4_16920.jpg  \n",
            "  inflating: data/training_images/vid_4_16940.jpg  \n",
            "  inflating: data/training_images/vid_4_16960.jpg  \n",
            "  inflating: data/training_images/vid_4_16980.jpg  \n",
            "  inflating: data/training_images/vid_4_1700.jpg  \n",
            "  inflating: data/training_images/vid_4_17000.jpg  \n",
            "  inflating: data/training_images/vid_4_17020.jpg  \n",
            "  inflating: data/training_images/vid_4_17040.jpg  \n",
            "  inflating: data/training_images/vid_4_17060.jpg  \n",
            "  inflating: data/training_images/vid_4_17080.jpg  \n",
            "  inflating: data/training_images/vid_4_17100.jpg  \n",
            "  inflating: data/training_images/vid_4_17120.jpg  \n",
            "  inflating: data/training_images/vid_4_17140.jpg  \n",
            "  inflating: data/training_images/vid_4_17160.jpg  \n",
            "  inflating: data/training_images/vid_4_17180.jpg  \n",
            "  inflating: data/training_images/vid_4_17200.jpg  \n",
            "  inflating: data/training_images/vid_4_17220.jpg  \n",
            "  inflating: data/training_images/vid_4_17240.jpg  \n",
            "  inflating: data/training_images/vid_4_17260.jpg  \n",
            "  inflating: data/training_images/vid_4_17280.jpg  \n",
            "  inflating: data/training_images/vid_4_17300.jpg  \n",
            "  inflating: data/training_images/vid_4_17320.jpg  \n",
            "  inflating: data/training_images/vid_4_17340.jpg  \n",
            "  inflating: data/training_images/vid_4_17360.jpg  \n",
            "  inflating: data/training_images/vid_4_17380.jpg  \n",
            "  inflating: data/training_images/vid_4_1740.jpg  \n",
            "  inflating: data/training_images/vid_4_17400.jpg  \n",
            "  inflating: data/training_images/vid_4_17420.jpg  \n",
            "  inflating: data/training_images/vid_4_17440.jpg  \n",
            "  inflating: data/training_images/vid_4_17460.jpg  \n",
            "  inflating: data/training_images/vid_4_17480.jpg  \n",
            "  inflating: data/training_images/vid_4_17500.jpg  \n",
            "  inflating: data/training_images/vid_4_17520.jpg  \n",
            "  inflating: data/training_images/vid_4_17540.jpg  \n",
            "  inflating: data/training_images/vid_4_17560.jpg  \n",
            "  inflating: data/training_images/vid_4_17580.jpg  \n",
            "  inflating: data/training_images/vid_4_1760.jpg  \n",
            "  inflating: data/training_images/vid_4_17600.jpg  \n",
            "  inflating: data/training_images/vid_4_17620.jpg  \n",
            "  inflating: data/training_images/vid_4_17640.jpg  \n",
            "  inflating: data/training_images/vid_4_17660.jpg  \n",
            "  inflating: data/training_images/vid_4_17680.jpg  \n",
            "  inflating: data/training_images/vid_4_17700.jpg  \n",
            "  inflating: data/training_images/vid_4_17720.jpg  \n",
            "  inflating: data/training_images/vid_4_17740.jpg  \n",
            "  inflating: data/training_images/vid_4_17760.jpg  \n",
            "  inflating: data/training_images/vid_4_17780.jpg  \n",
            "  inflating: data/training_images/vid_4_1780.jpg  \n",
            "  inflating: data/training_images/vid_4_17800.jpg  \n",
            "  inflating: data/training_images/vid_4_17820.jpg  \n",
            "  inflating: data/training_images/vid_4_17840.jpg  \n",
            "  inflating: data/training_images/vid_4_17860.jpg  \n",
            "  inflating: data/training_images/vid_4_17880.jpg  \n",
            "  inflating: data/training_images/vid_4_17900.jpg  \n",
            "  inflating: data/training_images/vid_4_17920.jpg  \n",
            "  inflating: data/training_images/vid_4_17940.jpg  \n",
            "  inflating: data/training_images/vid_4_17960.jpg  \n",
            "  inflating: data/training_images/vid_4_17980.jpg  \n",
            "  inflating: data/training_images/vid_4_1800.jpg  \n",
            "  inflating: data/training_images/vid_4_18000.jpg  \n",
            "  inflating: data/training_images/vid_4_18080.jpg  \n",
            "  inflating: data/training_images/vid_4_18100.jpg  \n",
            "  inflating: data/training_images/vid_4_18120.jpg  \n",
            "  inflating: data/training_images/vid_4_18140.jpg  \n",
            "  inflating: data/training_images/vid_4_18160.jpg  \n",
            "  inflating: data/training_images/vid_4_18180.jpg  \n",
            "  inflating: data/training_images/vid_4_1820.jpg  \n",
            "  inflating: data/training_images/vid_4_18200.jpg  \n",
            "  inflating: data/training_images/vid_4_18220.jpg  \n",
            "  inflating: data/training_images/vid_4_18240.jpg  \n",
            "  inflating: data/training_images/vid_4_18260.jpg  \n",
            "  inflating: data/training_images/vid_4_18280.jpg  \n",
            "  inflating: data/training_images/vid_4_18300.jpg  \n",
            "  inflating: data/training_images/vid_4_18320.jpg  \n",
            "  inflating: data/training_images/vid_4_18340.jpg  \n",
            "  inflating: data/training_images/vid_4_18360.jpg  \n",
            "  inflating: data/training_images/vid_4_18380.jpg  \n",
            "  inflating: data/training_images/vid_4_1840.jpg  \n",
            "  inflating: data/training_images/vid_4_18400.jpg  \n",
            "  inflating: data/training_images/vid_4_18420.jpg  \n",
            "  inflating: data/training_images/vid_4_18440.jpg  \n",
            "  inflating: data/training_images/vid_4_1860.jpg  \n",
            "  inflating: data/training_images/vid_4_18640.jpg  \n",
            "  inflating: data/training_images/vid_4_18660.jpg  \n",
            "  inflating: data/training_images/vid_4_18680.jpg  \n",
            "  inflating: data/training_images/vid_4_18700.jpg  \n",
            "  inflating: data/training_images/vid_4_18720.jpg  \n",
            "  inflating: data/training_images/vid_4_18740.jpg  \n",
            "  inflating: data/training_images/vid_4_18760.jpg  \n",
            "  inflating: data/training_images/vid_4_18780.jpg  \n",
            "  inflating: data/training_images/vid_4_1880.jpg  \n",
            "  inflating: data/training_images/vid_4_18800.jpg  \n",
            "  inflating: data/training_images/vid_4_18820.jpg  \n",
            "  inflating: data/training_images/vid_4_18840.jpg  \n",
            "  inflating: data/training_images/vid_4_18860.jpg  \n",
            "  inflating: data/training_images/vid_4_18880.jpg  \n",
            "  inflating: data/training_images/vid_4_18900.jpg  \n",
            "  inflating: data/training_images/vid_4_18920.jpg  \n",
            "  inflating: data/training_images/vid_4_18940.jpg  \n",
            "  inflating: data/training_images/vid_4_18960.jpg  \n",
            "  inflating: data/training_images/vid_4_18980.jpg  \n",
            "  inflating: data/training_images/vid_4_1900.jpg  \n",
            "  inflating: data/training_images/vid_4_19000.jpg  \n",
            "  inflating: data/training_images/vid_4_19020.jpg  \n",
            "  inflating: data/training_images/vid_4_19040.jpg  \n",
            "  inflating: data/training_images/vid_4_19060.jpg  \n",
            "  inflating: data/training_images/vid_4_19080.jpg  \n",
            "  inflating: data/training_images/vid_4_19100.jpg  \n",
            "  inflating: data/training_images/vid_4_19120.jpg  \n",
            "  inflating: data/training_images/vid_4_19140.jpg  \n",
            "  inflating: data/training_images/vid_4_19160.jpg  \n",
            "  inflating: data/training_images/vid_4_19180.jpg  \n",
            "  inflating: data/training_images/vid_4_1920.jpg  \n",
            "  inflating: data/training_images/vid_4_19200.jpg  \n",
            "  inflating: data/training_images/vid_4_19220.jpg  \n",
            "  inflating: data/training_images/vid_4_19240.jpg  \n",
            "  inflating: data/training_images/vid_4_19260.jpg  \n",
            "  inflating: data/training_images/vid_4_19280.jpg  \n",
            "  inflating: data/training_images/vid_4_19300.jpg  \n",
            "  inflating: data/training_images/vid_4_19320.jpg  \n",
            "  inflating: data/training_images/vid_4_19340.jpg  \n",
            "  inflating: data/training_images/vid_4_19360.jpg  \n",
            "  inflating: data/training_images/vid_4_19380.jpg  \n",
            "  inflating: data/training_images/vid_4_1940.jpg  \n",
            "  inflating: data/training_images/vid_4_19400.jpg  \n",
            "  inflating: data/training_images/vid_4_19420.jpg  \n",
            "  inflating: data/training_images/vid_4_19440.jpg  \n",
            "  inflating: data/training_images/vid_4_19460.jpg  \n",
            "  inflating: data/training_images/vid_4_19480.jpg  \n",
            "  inflating: data/training_images/vid_4_19500.jpg  \n",
            "  inflating: data/training_images/vid_4_19520.jpg  \n",
            "  inflating: data/training_images/vid_4_19540.jpg  \n",
            "  inflating: data/training_images/vid_4_19560.jpg  \n",
            "  inflating: data/training_images/vid_4_19580.jpg  \n",
            "  inflating: data/training_images/vid_4_1960.jpg  \n",
            "  inflating: data/training_images/vid_4_19600.jpg  \n",
            "  inflating: data/training_images/vid_4_19620.jpg  \n",
            "  inflating: data/training_images/vid_4_19640.jpg  \n",
            "  inflating: data/training_images/vid_4_19660.jpg  \n",
            "  inflating: data/training_images/vid_4_19680.jpg  \n",
            "  inflating: data/training_images/vid_4_19700.jpg  \n",
            "  inflating: data/training_images/vid_4_19720.jpg  \n",
            "  inflating: data/training_images/vid_4_19740.jpg  \n",
            "  inflating: data/training_images/vid_4_19760.jpg  \n",
            "  inflating: data/training_images/vid_4_19780.jpg  \n",
            "  inflating: data/training_images/vid_4_1980.jpg  \n",
            "  inflating: data/training_images/vid_4_19800.jpg  \n",
            "  inflating: data/training_images/vid_4_19820.jpg  \n",
            "  inflating: data/training_images/vid_4_19840.jpg  \n",
            "  inflating: data/training_images/vid_4_19860.jpg  \n",
            "  inflating: data/training_images/vid_4_19880.jpg  \n",
            "  inflating: data/training_images/vid_4_19900.jpg  \n",
            "  inflating: data/training_images/vid_4_19920.jpg  \n",
            "  inflating: data/training_images/vid_4_19940.jpg  \n",
            "  inflating: data/training_images/vid_4_19960.jpg  \n",
            "  inflating: data/training_images/vid_4_19980.jpg  \n",
            "  inflating: data/training_images/vid_4_2000.jpg  \n",
            "  inflating: data/training_images/vid_4_20000.jpg  \n",
            "  inflating: data/training_images/vid_4_20020.jpg  \n",
            "  inflating: data/training_images/vid_4_20040.jpg  \n",
            "  inflating: data/training_images/vid_4_20060.jpg  \n",
            "  inflating: data/training_images/vid_4_20080.jpg  \n",
            "  inflating: data/training_images/vid_4_20100.jpg  \n",
            "  inflating: data/training_images/vid_4_20120.jpg  \n",
            "  inflating: data/training_images/vid_4_20140.jpg  \n",
            "  inflating: data/training_images/vid_4_2020.jpg  \n",
            "  inflating: data/training_images/vid_4_20240.jpg  \n",
            "  inflating: data/training_images/vid_4_20260.jpg  \n",
            "  inflating: data/training_images/vid_4_20280.jpg  \n",
            "  inflating: data/training_images/vid_4_20300.jpg  \n",
            "  inflating: data/training_images/vid_4_20320.jpg  \n",
            "  inflating: data/training_images/vid_4_2040.jpg  \n",
            "  inflating: data/training_images/vid_4_20440.jpg  \n",
            "  inflating: data/training_images/vid_4_20460.jpg  \n",
            "  inflating: data/training_images/vid_4_20480.jpg  \n",
            "  inflating: data/training_images/vid_4_20500.jpg  \n",
            "  inflating: data/training_images/vid_4_20520.jpg  \n",
            "  inflating: data/training_images/vid_4_20540.jpg  \n",
            "  inflating: data/training_images/vid_4_20560.jpg  \n",
            "  inflating: data/training_images/vid_4_20580.jpg  \n",
            "  inflating: data/training_images/vid_4_2060.jpg  \n",
            "  inflating: data/training_images/vid_4_20780.jpg  \n",
            "  inflating: data/training_images/vid_4_2080.jpg  \n",
            "  inflating: data/training_images/vid_4_20800.jpg  \n",
            "  inflating: data/training_images/vid_4_20820.jpg  \n",
            "  inflating: data/training_images/vid_4_20840.jpg  \n",
            "  inflating: data/training_images/vid_4_20860.jpg  \n",
            "  inflating: data/training_images/vid_4_20880.jpg  \n",
            "  inflating: data/training_images/vid_4_20900.jpg  \n",
            "  inflating: data/training_images/vid_4_20920.jpg  \n",
            "  inflating: data/training_images/vid_4_20940.jpg  \n",
            "  inflating: data/training_images/vid_4_20960.jpg  \n",
            "  inflating: data/training_images/vid_4_20980.jpg  \n",
            "  inflating: data/training_images/vid_4_2100.jpg  \n",
            "  inflating: data/training_images/vid_4_21000.jpg  \n",
            "  inflating: data/training_images/vid_4_21020.jpg  \n",
            "  inflating: data/training_images/vid_4_21040.jpg  \n",
            "  inflating: data/training_images/vid_4_21060.jpg  \n",
            "  inflating: data/training_images/vid_4_21080.jpg  \n",
            "  inflating: data/training_images/vid_4_21100.jpg  \n",
            "  inflating: data/training_images/vid_4_21120.jpg  \n",
            "  inflating: data/training_images/vid_4_21140.jpg  \n",
            "  inflating: data/training_images/vid_4_21160.jpg  \n",
            "  inflating: data/training_images/vid_4_21180.jpg  \n",
            "  inflating: data/training_images/vid_4_2120.jpg  \n",
            "  inflating: data/training_images/vid_4_21200.jpg  \n",
            "  inflating: data/training_images/vid_4_21220.jpg  \n",
            "  inflating: data/training_images/vid_4_21240.jpg  \n",
            "  inflating: data/training_images/vid_4_21260.jpg  \n",
            "  inflating: data/training_images/vid_4_21280.jpg  \n",
            "  inflating: data/training_images/vid_4_21300.jpg  \n",
            "  inflating: data/training_images/vid_4_21320.jpg  \n",
            "  inflating: data/training_images/vid_4_21340.jpg  \n",
            "  inflating: data/training_images/vid_4_21360.jpg  \n",
            "  inflating: data/training_images/vid_4_21380.jpg  \n",
            "  inflating: data/training_images/vid_4_2140.jpg  \n",
            "  inflating: data/training_images/vid_4_21400.jpg  \n",
            "  inflating: data/training_images/vid_4_21420.jpg  \n",
            "  inflating: data/training_images/vid_4_21440.jpg  \n",
            "  inflating: data/training_images/vid_4_21460.jpg  \n",
            "  inflating: data/training_images/vid_4_21480.jpg  \n",
            "  inflating: data/training_images/vid_4_21500.jpg  \n",
            "  inflating: data/training_images/vid_4_21520.jpg  \n",
            "  inflating: data/training_images/vid_4_21540.jpg  \n",
            "  inflating: data/training_images/vid_4_21560.jpg  \n",
            "  inflating: data/training_images/vid_4_21580.jpg  \n",
            "  inflating: data/training_images/vid_4_2160.jpg  \n",
            "  inflating: data/training_images/vid_4_21600.jpg  \n",
            "  inflating: data/training_images/vid_4_21620.jpg  \n",
            "  inflating: data/training_images/vid_4_21640.jpg  \n",
            "  inflating: data/training_images/vid_4_21660.jpg  \n",
            "  inflating: data/training_images/vid_4_21680.jpg  \n",
            "  inflating: data/training_images/vid_4_21700.jpg  \n",
            "  inflating: data/training_images/vid_4_21720.jpg  \n",
            "  inflating: data/training_images/vid_4_21740.jpg  \n",
            "  inflating: data/training_images/vid_4_2180.jpg  \n",
            "  inflating: data/training_images/vid_4_21800.jpg  \n",
            "  inflating: data/training_images/vid_4_21820.jpg  \n",
            "  inflating: data/training_images/vid_4_21840.jpg  \n",
            "  inflating: data/training_images/vid_4_21860.jpg  \n",
            "  inflating: data/training_images/vid_4_21880.jpg  \n",
            "  inflating: data/training_images/vid_4_21900.jpg  \n",
            "  inflating: data/training_images/vid_4_21920.jpg  \n",
            "  inflating: data/training_images/vid_4_21940.jpg  \n",
            "  inflating: data/training_images/vid_4_21960.jpg  \n",
            "  inflating: data/training_images/vid_4_21980.jpg  \n",
            "  inflating: data/training_images/vid_4_2200.jpg  \n",
            "  inflating: data/training_images/vid_4_22000.jpg  \n",
            "  inflating: data/training_images/vid_4_22020.jpg  \n",
            "  inflating: data/training_images/vid_4_22040.jpg  \n",
            "  inflating: data/training_images/vid_4_22060.jpg  \n",
            "  inflating: data/training_images/vid_4_22080.jpg  \n",
            "  inflating: data/training_images/vid_4_22100.jpg  \n",
            "  inflating: data/training_images/vid_4_22120.jpg  \n",
            "  inflating: data/training_images/vid_4_22140.jpg  \n",
            "  inflating: data/training_images/vid_4_22160.jpg  \n",
            "  inflating: data/training_images/vid_4_22180.jpg  \n",
            "  inflating: data/training_images/vid_4_2220.jpg  \n",
            "  inflating: data/training_images/vid_4_22200.jpg  \n",
            "  inflating: data/training_images/vid_4_22220.jpg  \n",
            "  inflating: data/training_images/vid_4_22240.jpg  \n",
            "  inflating: data/training_images/vid_4_22260.jpg  \n",
            "  inflating: data/training_images/vid_4_22280.jpg  \n",
            "  inflating: data/training_images/vid_4_22300.jpg  \n",
            "  inflating: data/training_images/vid_4_22320.jpg  \n",
            "  inflating: data/training_images/vid_4_22340.jpg  \n",
            "  inflating: data/training_images/vid_4_22360.jpg  \n",
            "  inflating: data/training_images/vid_4_22380.jpg  \n",
            "  inflating: data/training_images/vid_4_2240.jpg  \n",
            "  inflating: data/training_images/vid_4_22400.jpg  \n",
            "  inflating: data/training_images/vid_4_22420.jpg  \n",
            "  inflating: data/training_images/vid_4_22440.jpg  \n",
            "  inflating: data/training_images/vid_4_22460.jpg  \n",
            "  inflating: data/training_images/vid_4_22480.jpg  \n",
            "  inflating: data/training_images/vid_4_22500.jpg  \n",
            "  inflating: data/training_images/vid_4_22520.jpg  \n",
            "  inflating: data/training_images/vid_4_22540.jpg  \n",
            "  inflating: data/training_images/vid_4_22560.jpg  \n",
            "  inflating: data/training_images/vid_4_22580.jpg  \n",
            "  inflating: data/training_images/vid_4_2260.jpg  \n",
            "  inflating: data/training_images/vid_4_22600.jpg  \n",
            "  inflating: data/training_images/vid_4_22620.jpg  \n",
            "  inflating: data/training_images/vid_4_22640.jpg  \n",
            "  inflating: data/training_images/vid_4_22660.jpg  \n",
            "  inflating: data/training_images/vid_4_22680.jpg  \n",
            "  inflating: data/training_images/vid_4_22700.jpg  \n",
            "  inflating: data/training_images/vid_4_22720.jpg  \n",
            "  inflating: data/training_images/vid_4_22740.jpg  \n",
            "  inflating: data/training_images/vid_4_22760.jpg  \n",
            "  inflating: data/training_images/vid_4_22780.jpg  \n",
            "  inflating: data/training_images/vid_4_2280.jpg  \n",
            "  inflating: data/training_images/vid_4_22800.jpg  \n",
            "  inflating: data/training_images/vid_4_22820.jpg  \n",
            "  inflating: data/training_images/vid_4_22840.jpg  \n",
            "  inflating: data/training_images/vid_4_22860.jpg  \n",
            "  inflating: data/training_images/vid_4_22880.jpg  \n",
            "  inflating: data/training_images/vid_4_22900.jpg  \n",
            "  inflating: data/training_images/vid_4_22920.jpg  \n",
            "  inflating: data/training_images/vid_4_22940.jpg  \n",
            "  inflating: data/training_images/vid_4_22960.jpg  \n",
            "  inflating: data/training_images/vid_4_22980.jpg  \n",
            "  inflating: data/training_images/vid_4_2300.jpg  \n",
            "  inflating: data/training_images/vid_4_23000.jpg  \n",
            "  inflating: data/training_images/vid_4_23020.jpg  \n",
            "  inflating: data/training_images/vid_4_23040.jpg  \n",
            "  inflating: data/training_images/vid_4_23060.jpg  \n",
            "  inflating: data/training_images/vid_4_23080.jpg  \n",
            "  inflating: data/training_images/vid_4_23100.jpg  \n",
            "  inflating: data/training_images/vid_4_23120.jpg  \n",
            "  inflating: data/training_images/vid_4_23140.jpg  \n",
            "  inflating: data/training_images/vid_4_23160.jpg  \n",
            "  inflating: data/training_images/vid_4_23180.jpg  \n",
            "  inflating: data/training_images/vid_4_2320.jpg  \n",
            "  inflating: data/training_images/vid_4_23200.jpg  \n",
            "  inflating: data/training_images/vid_4_23220.jpg  \n",
            "  inflating: data/training_images/vid_4_23240.jpg  \n",
            "  inflating: data/training_images/vid_4_23260.jpg  \n",
            "  inflating: data/training_images/vid_4_23280.jpg  \n",
            "  inflating: data/training_images/vid_4_23300.jpg  \n",
            "  inflating: data/training_images/vid_4_23320.jpg  \n",
            "  inflating: data/training_images/vid_4_23340.jpg  \n",
            "  inflating: data/training_images/vid_4_23360.jpg  \n",
            "  inflating: data/training_images/vid_4_23380.jpg  \n",
            "  inflating: data/training_images/vid_4_2340.jpg  \n",
            "  inflating: data/training_images/vid_4_23400.jpg  \n",
            "  inflating: data/training_images/vid_4_23420.jpg  \n",
            "  inflating: data/training_images/vid_4_23440.jpg  \n",
            "  inflating: data/training_images/vid_4_23460.jpg  \n",
            "  inflating: data/training_images/vid_4_23480.jpg  \n",
            "  inflating: data/training_images/vid_4_23500.jpg  \n",
            "  inflating: data/training_images/vid_4_23520.jpg  \n",
            "  inflating: data/training_images/vid_4_23540.jpg  \n",
            "  inflating: data/training_images/vid_4_23560.jpg  \n",
            "  inflating: data/training_images/vid_4_23580.jpg  \n",
            "  inflating: data/training_images/vid_4_2360.jpg  \n",
            "  inflating: data/training_images/vid_4_23600.jpg  \n",
            "  inflating: data/training_images/vid_4_23620.jpg  \n",
            "  inflating: data/training_images/vid_4_23640.jpg  \n",
            "  inflating: data/training_images/vid_4_2380.jpg  \n",
            "  inflating: data/training_images/vid_4_2400.jpg  \n",
            "  inflating: data/training_images/vid_4_2420.jpg  \n",
            "  inflating: data/training_images/vid_4_2440.jpg  \n",
            "  inflating: data/training_images/vid_4_2460.jpg  \n",
            "  inflating: data/training_images/vid_4_24760.jpg  \n",
            "  inflating: data/training_images/vid_4_24780.jpg  \n",
            "  inflating: data/training_images/vid_4_2480.jpg  \n",
            "  inflating: data/training_images/vid_4_24800.jpg  \n",
            "  inflating: data/training_images/vid_4_24820.jpg  \n",
            "  inflating: data/training_images/vid_4_24840.jpg  \n",
            "  inflating: data/training_images/vid_4_24860.jpg  \n",
            "  inflating: data/training_images/vid_4_24880.jpg  \n",
            "  inflating: data/training_images/vid_4_24900.jpg  \n",
            "  inflating: data/training_images/vid_4_24920.jpg  \n",
            "  inflating: data/training_images/vid_4_24940.jpg  \n",
            "  inflating: data/training_images/vid_4_24960.jpg  \n",
            "  inflating: data/training_images/vid_4_2500.jpg  \n",
            "  inflating: data/training_images/vid_4_2520.jpg  \n",
            "  inflating: data/training_images/vid_4_2540.jpg  \n",
            "  inflating: data/training_images/vid_4_2560.jpg  \n",
            "  inflating: data/training_images/vid_4_25820.jpg  \n",
            "  inflating: data/training_images/vid_4_25840.jpg  \n",
            "  inflating: data/training_images/vid_4_25860.jpg  \n",
            "  inflating: data/training_images/vid_4_25880.jpg  \n",
            "  inflating: data/training_images/vid_4_25900.jpg  \n",
            "  inflating: data/training_images/vid_4_25920.jpg  \n",
            "  inflating: data/training_images/vid_4_25940.jpg  \n",
            "  inflating: data/training_images/vid_4_25960.jpg  \n",
            "  inflating: data/training_images/vid_4_25980.jpg  \n",
            "  inflating: data/training_images/vid_4_26000.jpg  \n",
            "  inflating: data/training_images/vid_4_26020.jpg  \n",
            "  inflating: data/training_images/vid_4_26040.jpg  \n",
            "  inflating: data/training_images/vid_4_26060.jpg  \n",
            "  inflating: data/training_images/vid_4_26080.jpg  \n",
            "  inflating: data/training_images/vid_4_26100.jpg  \n",
            "  inflating: data/training_images/vid_4_26120.jpg  \n",
            "  inflating: data/training_images/vid_4_26140.jpg  \n",
            "  inflating: data/training_images/vid_4_26160.jpg  \n",
            "  inflating: data/training_images/vid_4_26180.jpg  \n",
            "  inflating: data/training_images/vid_4_26200.jpg  \n",
            "  inflating: data/training_images/vid_4_26220.jpg  \n",
            "  inflating: data/training_images/vid_4_26240.jpg  \n",
            "  inflating: data/training_images/vid_4_26260.jpg  \n",
            "  inflating: data/training_images/vid_4_26280.jpg  \n",
            "  inflating: data/training_images/vid_4_26300.jpg  \n",
            "  inflating: data/training_images/vid_4_26320.jpg  \n",
            "  inflating: data/training_images/vid_4_26340.jpg  \n",
            "  inflating: data/training_images/vid_4_26360.jpg  \n",
            "  inflating: data/training_images/vid_4_26380.jpg  \n",
            "  inflating: data/training_images/vid_4_26400.jpg  \n",
            "  inflating: data/training_images/vid_4_26420.jpg  \n",
            "  inflating: data/training_images/vid_4_26440.jpg  \n",
            "  inflating: data/training_images/vid_4_26460.jpg  \n",
            "  inflating: data/training_images/vid_4_26480.jpg  \n",
            "  inflating: data/training_images/vid_4_26500.jpg  \n",
            "  inflating: data/training_images/vid_4_26520.jpg  \n",
            "  inflating: data/training_images/vid_4_26540.jpg  \n",
            "  inflating: data/training_images/vid_4_26560.jpg  \n",
            "  inflating: data/training_images/vid_4_26580.jpg  \n",
            "  inflating: data/training_images/vid_4_26600.jpg  \n",
            "  inflating: data/training_images/vid_4_26620.jpg  \n",
            "  inflating: data/training_images/vid_4_26640.jpg  \n",
            "  inflating: data/training_images/vid_4_26660.jpg  \n",
            "  inflating: data/training_images/vid_4_26680.jpg  \n",
            "  inflating: data/training_images/vid_4_26700.jpg  \n",
            "  inflating: data/training_images/vid_4_26720.jpg  \n",
            "  inflating: data/training_images/vid_4_26740.jpg  \n",
            "  inflating: data/training_images/vid_4_26760.jpg  \n",
            "  inflating: data/training_images/vid_4_26780.jpg  \n",
            "  inflating: data/training_images/vid_4_27040.jpg  \n",
            "  inflating: data/training_images/vid_4_27060.jpg  \n",
            "  inflating: data/training_images/vid_4_27080.jpg  \n",
            "  inflating: data/training_images/vid_4_27100.jpg  \n",
            "  inflating: data/training_images/vid_4_27120.jpg  \n",
            "  inflating: data/training_images/vid_4_27140.jpg  \n",
            "  inflating: data/training_images/vid_4_27160.jpg  \n",
            "  inflating: data/training_images/vid_4_27180.jpg  \n",
            "  inflating: data/training_images/vid_4_27200.jpg  \n",
            "  inflating: data/training_images/vid_4_27220.jpg  \n",
            "  inflating: data/training_images/vid_4_27240.jpg  \n",
            "  inflating: data/training_images/vid_4_27260.jpg  \n",
            "  inflating: data/training_images/vid_4_27280.jpg  \n",
            "  inflating: data/training_images/vid_4_28200.jpg  \n",
            "  inflating: data/training_images/vid_4_28220.jpg  \n",
            "  inflating: data/training_images/vid_4_28240.jpg  \n",
            "  inflating: data/training_images/vid_4_28260.jpg  \n",
            "  inflating: data/training_images/vid_4_28280.jpg  \n",
            "  inflating: data/training_images/vid_4_28300.jpg  \n",
            "  inflating: data/training_images/vid_4_28320.jpg  \n",
            "  inflating: data/training_images/vid_4_28340.jpg  \n",
            "  inflating: data/training_images/vid_4_28360.jpg  \n",
            "  inflating: data/training_images/vid_4_28380.jpg  \n",
            "  inflating: data/training_images/vid_4_28400.jpg  \n",
            "  inflating: data/training_images/vid_4_28420.jpg  \n",
            "  inflating: data/training_images/vid_4_28440.jpg  \n",
            "  inflating: data/training_images/vid_4_28820.jpg  \n",
            "  inflating: data/training_images/vid_4_28840.jpg  \n",
            "  inflating: data/training_images/vid_4_28860.jpg  \n",
            "  inflating: data/training_images/vid_4_28880.jpg  \n",
            "  inflating: data/training_images/vid_4_29280.jpg  \n",
            "  inflating: data/training_images/vid_4_29300.jpg  \n",
            "  inflating: data/training_images/vid_4_29320.jpg  \n",
            "  inflating: data/training_images/vid_4_29340.jpg  \n",
            "  inflating: data/training_images/vid_4_29360.jpg  \n",
            "  inflating: data/training_images/vid_4_29380.jpg  \n",
            "  inflating: data/training_images/vid_4_29400.jpg  \n",
            "  inflating: data/training_images/vid_4_29420.jpg  \n",
            "  inflating: data/training_images/vid_4_29440.jpg  \n",
            "  inflating: data/training_images/vid_4_29460.jpg  \n",
            "  inflating: data/training_images/vid_4_29480.jpg  \n",
            "  inflating: data/training_images/vid_4_29500.jpg  \n",
            "  inflating: data/training_images/vid_4_29520.jpg  \n",
            "  inflating: data/training_images/vid_4_29540.jpg  \n",
            "  inflating: data/training_images/vid_4_29560.jpg  \n",
            "  inflating: data/training_images/vid_4_29580.jpg  \n",
            "  inflating: data/training_images/vid_4_29600.jpg  \n",
            "  inflating: data/training_images/vid_4_29620.jpg  \n",
            "  inflating: data/training_images/vid_4_29640.jpg  \n",
            "  inflating: data/training_images/vid_4_29660.jpg  \n",
            "  inflating: data/training_images/vid_4_29680.jpg  \n",
            "  inflating: data/training_images/vid_4_29700.jpg  \n",
            "  inflating: data/training_images/vid_4_29720.jpg  \n",
            "  inflating: data/training_images/vid_4_29740.jpg  \n",
            "  inflating: data/training_images/vid_4_29760.jpg  \n",
            "  inflating: data/training_images/vid_4_29880.jpg  \n",
            "  inflating: data/training_images/vid_4_29900.jpg  \n",
            "  inflating: data/training_images/vid_4_29920.jpg  \n",
            "  inflating: data/training_images/vid_4_29940.jpg  \n",
            "  inflating: data/training_images/vid_4_29960.jpg  \n",
            "  inflating: data/training_images/vid_4_29980.jpg  \n",
            "  inflating: data/training_images/vid_4_30000.jpg  \n",
            "  inflating: data/training_images/vid_4_30020.jpg  \n",
            "  inflating: data/training_images/vid_4_30440.jpg  \n",
            "  inflating: data/training_images/vid_4_3120.jpg  \n",
            "  inflating: data/training_images/vid_4_3140.jpg  \n",
            "  inflating: data/training_images/vid_4_3160.jpg  \n",
            "  inflating: data/training_images/vid_4_3180.jpg  \n",
            "  inflating: data/training_images/vid_4_3200.jpg  \n",
            "  inflating: data/training_images/vid_4_3220.jpg  \n",
            "  inflating: data/training_images/vid_4_3240.jpg  \n",
            "  inflating: data/training_images/vid_4_3260.jpg  \n",
            "  inflating: data/training_images/vid_4_3280.jpg  \n",
            "  inflating: data/training_images/vid_4_3300.jpg  \n",
            "  inflating: data/training_images/vid_4_3320.jpg  \n",
            "  inflating: data/training_images/vid_4_3340.jpg  \n",
            "  inflating: data/training_images/vid_4_3360.jpg  \n",
            "  inflating: data/training_images/vid_4_3380.jpg  \n",
            "  inflating: data/training_images/vid_4_3400.jpg  \n",
            "  inflating: data/training_images/vid_4_3420.jpg  \n",
            "  inflating: data/training_images/vid_4_3440.jpg  \n",
            "  inflating: data/training_images/vid_4_3460.jpg  \n",
            "  inflating: data/training_images/vid_4_3480.jpg  \n",
            "  inflating: data/training_images/vid_4_3500.jpg  \n",
            "  inflating: data/training_images/vid_4_3520.jpg  \n",
            "  inflating: data/training_images/vid_4_3540.jpg  \n",
            "  inflating: data/training_images/vid_4_3560.jpg  \n",
            "  inflating: data/training_images/vid_4_3580.jpg  \n",
            "  inflating: data/training_images/vid_4_3600.jpg  \n",
            "  inflating: data/training_images/vid_4_3620.jpg  \n",
            "  inflating: data/training_images/vid_4_3640.jpg  \n",
            "  inflating: data/training_images/vid_4_3660.jpg  \n",
            "  inflating: data/training_images/vid_4_3680.jpg  \n",
            "  inflating: data/training_images/vid_4_3700.jpg  \n",
            "  inflating: data/training_images/vid_4_3720.jpg  \n",
            "  inflating: data/training_images/vid_4_3740.jpg  \n",
            "  inflating: data/training_images/vid_4_3760.jpg  \n",
            "  inflating: data/training_images/vid_4_3780.jpg  \n",
            "  inflating: data/training_images/vid_4_3800.jpg  \n",
            "  inflating: data/training_images/vid_4_3820.jpg  \n",
            "  inflating: data/training_images/vid_4_3840.jpg  \n",
            "  inflating: data/training_images/vid_4_3860.jpg  \n",
            "  inflating: data/training_images/vid_4_3880.jpg  \n",
            "  inflating: data/training_images/vid_4_3900.jpg  \n",
            "  inflating: data/training_images/vid_4_3920.jpg  \n",
            "  inflating: data/training_images/vid_4_3940.jpg  \n",
            "  inflating: data/training_images/vid_4_3960.jpg  \n",
            "  inflating: data/training_images/vid_4_3980.jpg  \n",
            "  inflating: data/training_images/vid_4_4000.jpg  \n",
            "  inflating: data/training_images/vid_4_4020.jpg  \n",
            "  inflating: data/training_images/vid_4_4040.jpg  \n",
            "  inflating: data/training_images/vid_4_4060.jpg  \n",
            "  inflating: data/training_images/vid_4_4080.jpg  \n",
            "  inflating: data/training_images/vid_4_4100.jpg  \n",
            "  inflating: data/training_images/vid_4_4120.jpg  \n",
            "  inflating: data/training_images/vid_4_4140.jpg  \n",
            "  inflating: data/training_images/vid_4_4160.jpg  \n",
            "  inflating: data/training_images/vid_4_4180.jpg  \n",
            "  inflating: data/training_images/vid_4_4380.jpg  \n",
            "  inflating: data/training_images/vid_4_4400.jpg  \n",
            "  inflating: data/training_images/vid_4_4420.jpg  \n",
            "  inflating: data/training_images/vid_4_4520.jpg  \n",
            "  inflating: data/training_images/vid_4_4540.jpg  \n",
            "  inflating: data/training_images/vid_4_4560.jpg  \n",
            "  inflating: data/training_images/vid_4_4580.jpg  \n",
            "  inflating: data/training_images/vid_4_4600.jpg  \n",
            "  inflating: data/training_images/vid_4_4620.jpg  \n",
            "  inflating: data/training_images/vid_4_4640.jpg  \n",
            "  inflating: data/training_images/vid_4_5640.jpg  \n",
            "  inflating: data/training_images/vid_4_5660.jpg  \n",
            "  inflating: data/training_images/vid_4_5680.jpg  \n",
            "  inflating: data/training_images/vid_4_5700.jpg  \n",
            "  inflating: data/training_images/vid_4_5720.jpg  \n",
            "  inflating: data/training_images/vid_4_5740.jpg  \n",
            "  inflating: data/training_images/vid_4_5760.jpg  \n",
            "  inflating: data/training_images/vid_4_5780.jpg  \n",
            "  inflating: data/training_images/vid_4_5800.jpg  \n",
            "  inflating: data/training_images/vid_4_5820.jpg  \n",
            "  inflating: data/training_images/vid_4_5840.jpg  \n",
            "  inflating: data/training_images/vid_4_5860.jpg  \n",
            "  inflating: data/training_images/vid_4_5880.jpg  \n",
            "  inflating: data/training_images/vid_4_5900.jpg  \n",
            "  inflating: data/training_images/vid_4_5920.jpg  \n",
            "  inflating: data/training_images/vid_4_600.jpg  \n",
            "  inflating: data/training_images/vid_4_6080.jpg  \n",
            "  inflating: data/training_images/vid_4_6100.jpg  \n",
            "  inflating: data/training_images/vid_4_6120.jpg  \n",
            "  inflating: data/training_images/vid_4_6140.jpg  \n",
            "  inflating: data/training_images/vid_4_6160.jpg  \n",
            "  inflating: data/training_images/vid_4_6180.jpg  \n",
            "  inflating: data/training_images/vid_4_620.jpg  \n",
            "  inflating: data/training_images/vid_4_6200.jpg  \n",
            "  inflating: data/training_images/vid_4_6220.jpg  \n",
            "  inflating: data/training_images/vid_4_6240.jpg  \n",
            "  inflating: data/training_images/vid_4_6260.jpg  \n",
            "  inflating: data/training_images/vid_4_6280.jpg  \n",
            "  inflating: data/training_images/vid_4_6300.jpg  \n",
            "  inflating: data/training_images/vid_4_6320.jpg  \n",
            "  inflating: data/training_images/vid_4_6340.jpg  \n",
            "  inflating: data/training_images/vid_4_6360.jpg  \n",
            "  inflating: data/training_images/vid_4_6380.jpg  \n",
            "  inflating: data/training_images/vid_4_640.jpg  \n",
            "  inflating: data/training_images/vid_4_6400.jpg  \n",
            "  inflating: data/training_images/vid_4_6420.jpg  \n",
            "  inflating: data/training_images/vid_4_6440.jpg  \n",
            "  inflating: data/training_images/vid_4_6460.jpg  \n",
            "  inflating: data/training_images/vid_4_6480.jpg  \n",
            "  inflating: data/training_images/vid_4_6500.jpg  \n",
            "  inflating: data/training_images/vid_4_6520.jpg  \n",
            "  inflating: data/training_images/vid_4_6540.jpg  \n",
            "  inflating: data/training_images/vid_4_6560.jpg  \n",
            "  inflating: data/training_images/vid_4_6580.jpg  \n",
            "  inflating: data/training_images/vid_4_660.jpg  \n",
            "  inflating: data/training_images/vid_4_6600.jpg  \n",
            "  inflating: data/training_images/vid_4_6780.jpg  \n",
            "  inflating: data/training_images/vid_4_680.jpg  \n",
            "  inflating: data/training_images/vid_4_6800.jpg  \n",
            "  inflating: data/training_images/vid_4_6820.jpg  \n",
            "  inflating: data/training_images/vid_4_6880.jpg  \n",
            "  inflating: data/training_images/vid_4_6900.jpg  \n",
            "  inflating: data/training_images/vid_4_6920.jpg  \n",
            "  inflating: data/training_images/vid_4_6940.jpg  \n",
            "  inflating: data/training_images/vid_4_6960.jpg  \n",
            "  inflating: data/training_images/vid_4_6980.jpg  \n",
            "  inflating: data/training_images/vid_4_700.jpg  \n",
            "  inflating: data/training_images/vid_4_7000.jpg  \n",
            "  inflating: data/training_images/vid_4_7020.jpg  \n",
            "  inflating: data/training_images/vid_4_7040.jpg  \n",
            "  inflating: data/training_images/vid_4_7060.jpg  \n",
            "  inflating: data/training_images/vid_4_7080.jpg  \n",
            "  inflating: data/training_images/vid_4_7100.jpg  \n",
            "  inflating: data/training_images/vid_4_7120.jpg  \n",
            "  inflating: data/training_images/vid_4_7140.jpg  \n",
            "  inflating: data/training_images/vid_4_7160.jpg  \n",
            "  inflating: data/training_images/vid_4_7180.jpg  \n",
            "  inflating: data/training_images/vid_4_720.jpg  \n",
            "  inflating: data/training_images/vid_4_7200.jpg  \n",
            "  inflating: data/training_images/vid_4_7220.jpg  \n",
            "  inflating: data/training_images/vid_4_7240.jpg  \n",
            "  inflating: data/training_images/vid_4_7260.jpg  \n",
            "  inflating: data/training_images/vid_4_7280.jpg  \n",
            "  inflating: data/training_images/vid_4_7300.jpg  \n",
            "  inflating: data/training_images/vid_4_7320.jpg  \n",
            "  inflating: data/training_images/vid_4_7340.jpg  \n",
            "  inflating: data/training_images/vid_4_7360.jpg  \n",
            "  inflating: data/training_images/vid_4_7380.jpg  \n",
            "  inflating: data/training_images/vid_4_740.jpg  \n",
            "  inflating: data/training_images/vid_4_7400.jpg  \n",
            "  inflating: data/training_images/vid_4_7420.jpg  \n",
            "  inflating: data/training_images/vid_4_7440.jpg  \n",
            "  inflating: data/training_images/vid_4_7460.jpg  \n",
            "  inflating: data/training_images/vid_4_7480.jpg  \n",
            "  inflating: data/training_images/vid_4_7500.jpg  \n",
            "  inflating: data/training_images/vid_4_7520.jpg  \n",
            "  inflating: data/training_images/vid_4_7540.jpg  \n",
            "  inflating: data/training_images/vid_4_7560.jpg  \n",
            "  inflating: data/training_images/vid_4_7580.jpg  \n",
            "  inflating: data/training_images/vid_4_760.jpg  \n",
            "  inflating: data/training_images/vid_4_7600.jpg  \n",
            "  inflating: data/training_images/vid_4_7620.jpg  \n",
            "  inflating: data/training_images/vid_4_7640.jpg  \n",
            "  inflating: data/training_images/vid_4_7660.jpg  \n",
            "  inflating: data/training_images/vid_4_7680.jpg  \n",
            "  inflating: data/training_images/vid_4_7700.jpg  \n",
            "  inflating: data/training_images/vid_4_7720.jpg  \n",
            "  inflating: data/training_images/vid_4_7740.jpg  \n",
            "  inflating: data/training_images/vid_4_7760.jpg  \n",
            "  inflating: data/training_images/vid_4_780.jpg  \n",
            "  inflating: data/training_images/vid_4_800.jpg  \n",
            "  inflating: data/training_images/vid_4_820.jpg  \n",
            "  inflating: data/training_images/vid_4_8220.jpg  \n",
            "  inflating: data/training_images/vid_4_8240.jpg  \n",
            "  inflating: data/training_images/vid_4_8260.jpg  \n",
            "  inflating: data/training_images/vid_4_8280.jpg  \n",
            "  inflating: data/training_images/vid_4_8300.jpg  \n",
            "  inflating: data/training_images/vid_4_8320.jpg  \n",
            "  inflating: data/training_images/vid_4_8340.jpg  \n",
            "  inflating: data/training_images/vid_4_8560.jpg  \n",
            "  inflating: data/training_images/vid_4_8580.jpg  \n",
            "  inflating: data/training_images/vid_4_860.jpg  \n",
            "  inflating: data/training_images/vid_4_8600.jpg  \n",
            "  inflating: data/training_images/vid_4_8620.jpg  \n",
            "  inflating: data/training_images/vid_4_8640.jpg  \n",
            "  inflating: data/training_images/vid_4_8660.jpg  \n",
            "  inflating: data/training_images/vid_4_8680.jpg  \n",
            "  inflating: data/training_images/vid_4_8700.jpg  \n",
            "  inflating: data/training_images/vid_4_8720.jpg  \n",
            "  inflating: data/training_images/vid_4_8740.jpg  \n",
            "  inflating: data/training_images/vid_4_8760.jpg  \n",
            "  inflating: data/training_images/vid_4_8780.jpg  \n",
            "  inflating: data/training_images/vid_4_880.jpg  \n",
            "  inflating: data/training_images/vid_4_8800.jpg  \n",
            "  inflating: data/training_images/vid_4_8960.jpg  \n",
            "  inflating: data/training_images/vid_4_8980.jpg  \n",
            "  inflating: data/training_images/vid_4_900.jpg  \n",
            "  inflating: data/training_images/vid_4_9000.jpg  \n",
            "  inflating: data/training_images/vid_4_9020.jpg  \n",
            "  inflating: data/training_images/vid_4_9040.jpg  \n",
            "  inflating: data/training_images/vid_4_9060.jpg  \n",
            "  inflating: data/training_images/vid_4_9080.jpg  \n",
            "  inflating: data/training_images/vid_4_9100.jpg  \n",
            "  inflating: data/training_images/vid_4_9120.jpg  \n",
            "  inflating: data/training_images/vid_4_9140.jpg  \n",
            "  inflating: data/training_images/vid_4_9160.jpg  \n",
            "  inflating: data/training_images/vid_4_9180.jpg  \n",
            "  inflating: data/training_images/vid_4_920.jpg  \n",
            "  inflating: data/training_images/vid_4_9200.jpg  \n",
            "  inflating: data/training_images/vid_4_9220.jpg  \n",
            "  inflating: data/training_images/vid_4_9240.jpg  \n",
            "  inflating: data/training_images/vid_4_9260.jpg  \n",
            "  inflating: data/training_images/vid_4_9280.jpg  \n",
            "  inflating: data/training_images/vid_4_9300.jpg  \n",
            "  inflating: data/training_images/vid_4_9320.jpg  \n",
            "  inflating: data/training_images/vid_4_9340.jpg  \n",
            "  inflating: data/training_images/vid_4_9360.jpg  \n",
            "  inflating: data/training_images/vid_4_9380.jpg  \n",
            "  inflating: data/training_images/vid_4_940.jpg  \n",
            "  inflating: data/training_images/vid_4_9400.jpg  \n",
            "  inflating: data/training_images/vid_4_9420.jpg  \n",
            "  inflating: data/training_images/vid_4_9440.jpg  \n",
            "  inflating: data/training_images/vid_4_9460.jpg  \n",
            "  inflating: data/training_images/vid_4_9480.jpg  \n",
            "  inflating: data/training_images/vid_4_9500.jpg  \n",
            "  inflating: data/training_images/vid_4_9520.jpg  \n",
            "  inflating: data/training_images/vid_4_9540.jpg  \n",
            "  inflating: data/training_images/vid_4_9560.jpg  \n",
            "  inflating: data/training_images/vid_4_9580.jpg  \n",
            "  inflating: data/training_images/vid_4_960.jpg  \n",
            "  inflating: data/training_images/vid_4_9600.jpg  \n",
            "  inflating: data/training_images/vid_4_9620.jpg  \n",
            "  inflating: data/training_images/vid_4_9640.jpg  \n",
            "  inflating: data/training_images/vid_4_9660.jpg  \n",
            "  inflating: data/training_images/vid_4_9680.jpg  \n",
            "  inflating: data/training_images/vid_4_9700.jpg  \n",
            "  inflating: data/training_images/vid_4_9720.jpg  \n",
            "  inflating: data/training_images/vid_4_9740.jpg  \n",
            "  inflating: data/training_images/vid_4_9760.jpg  \n",
            "  inflating: data/training_images/vid_4_9780.jpg  \n",
            "  inflating: data/training_images/vid_4_980.jpg  \n",
            "  inflating: data/training_images/vid_4_9800.jpg  \n",
            "  inflating: data/training_images/vid_4_9820.jpg  \n",
            "  inflating: data/training_images/vid_4_9840.jpg  \n",
            "  inflating: data/training_images/vid_4_9860.jpg  \n",
            "  inflating: data/training_images/vid_4_9880.jpg  \n",
            "  inflating: data/training_images/vid_4_9900.jpg  \n",
            "  inflating: data/training_images/vid_4_9920.jpg  \n",
            "  inflating: data/training_images/vid_4_9940.jpg  \n",
            "  inflating: data/training_images/vid_4_9960.jpg  \n",
            "  inflating: data/training_images/vid_4_9980.jpg  \n"
          ]
        }
      ],
      "source": [
        "# unzip the dataset\n",
        "!unzip /content/car-object-detection.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1NsBHB8cvR4"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1SkIlqPfEXC"
      },
      "source": [
        "## Download pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFChpThga9rH",
        "outputId": "63221d4f-245d-47a7-ce1d-8bee361facc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/models/research\n"
          ]
        }
      ],
      "source": [
        "%cd /content/models/research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8hL2QRFfDEr",
        "outputId": "6672aa8b-f643-4cfe-c4ba-c895285ab6e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-10-02 06:59:24--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.111.128, 2607:f8b0:4004:c19::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.111.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 244817203 (233M) [application/x-tar]\n",
            "Saving to: ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’\n",
            "\n",
            "ssd_resnet50_v1_fpn 100%[===================>] 233.48M   190MB/s    in 1.2s    \n",
            "\n",
            "2022-10-02 06:59:26 (190 MB/s) - ‘ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz’ saved [244817203/244817203]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the model from model zoo fo tensorflow 2\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnHMX11bfgx9",
        "outputId": "ca040a89-3776-4605-db6f-4407eaa11a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/checkpoint\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0.index\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model/\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model/saved_model.pb\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model/assets/\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model/variables/\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
            "ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/saved_model/variables/variables.index\n"
          ]
        }
      ],
      "source": [
        "# Extracting the tar file \n",
        "!tar -xvf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc9cVHgWfy6M"
      },
      "outputs": [],
      "source": [
        "# Rename the folder as its a long name \n",
        "!mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8 retinanet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZVFY84jgG44"
      },
      "outputs": [],
      "source": [
        "# create a new training folder that will keep the checkpoints, saved models and config file for our new \n",
        "# trained model. \n",
        "\n",
        "!mkdir training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Xp6Qsjw8glyt",
        "outputId": "2a72ee24-dd90-4b19-d4ed-5d6cd0924749"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nitem{\\n  id:1\\n  name:'car'\\n}\\n\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generally we keep two things in the training folder : one is labelmap.pbtxt and other is config file \n",
        "# Now we need to manually create the labelmap.pbtxt file. In this case we just have one class and hence we will create\n",
        "# one class in our labelmap file. \n",
        "'''\n",
        "item{\n",
        "  id:1\n",
        "  name:'car'\n",
        "}\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLQMW5U0jfDM"
      },
      "source": [
        "## Generating TFRecords from CSV "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDqV2GNRqIaP"
      },
      "source": [
        "### Getting the CSV input ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EpkMz0lp8OG"
      },
      "outputs": [],
      "source": [
        "# Getting the csv input ready \n",
        "df_basic = pd.read_csv(r\"/content/data/train_solution_bounding_boxes (1).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BKrsGCPWqHdT",
        "outputId": "1f4fe779-271a-42ea-e8aa-0bbfc047b1bd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-de30779c-1bf3-4909-86b1-d197fadeb7e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vid_4_1000.jpg</td>\n",
              "      <td>281.259045</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>327.727931</td>\n",
              "      <td>223.225547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vid_4_10000.jpg</td>\n",
              "      <td>15.163531</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>120.329957</td>\n",
              "      <td>236.430180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vid_4_10040.jpg</td>\n",
              "      <td>239.192475</td>\n",
              "      <td>176.764801</td>\n",
              "      <td>361.968162</td>\n",
              "      <td>236.430180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vid_4_10020.jpg</td>\n",
              "      <td>496.483358</td>\n",
              "      <td>172.363256</td>\n",
              "      <td>630.020260</td>\n",
              "      <td>231.539575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vid_4_10060.jpg</td>\n",
              "      <td>16.630970</td>\n",
              "      <td>186.546010</td>\n",
              "      <td>132.558611</td>\n",
              "      <td>238.386422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>vid_4_9860.jpg</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>198.321729</td>\n",
              "      <td>49.235251</td>\n",
              "      <td>236.223284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>vid_4_9880.jpg</td>\n",
              "      <td>329.876184</td>\n",
              "      <td>156.482351</td>\n",
              "      <td>536.664239</td>\n",
              "      <td>250.497895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>vid_4_9900.jpg</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>168.295823</td>\n",
              "      <td>141.797524</td>\n",
              "      <td>239.176652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>vid_4_9960.jpg</td>\n",
              "      <td>487.428988</td>\n",
              "      <td>172.233646</td>\n",
              "      <td>616.917699</td>\n",
              "      <td>228.839864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>vid_4_9980.jpg</td>\n",
              "      <td>221.558631</td>\n",
              "      <td>182.570434</td>\n",
              "      <td>348.585579</td>\n",
              "      <td>238.192196</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>559 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de30779c-1bf3-4909-86b1-d197fadeb7e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-de30779c-1bf3-4909-86b1-d197fadeb7e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-de30779c-1bf3-4909-86b1-d197fadeb7e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               image        xmin        ymin        xmax        ymax\n",
              "0     vid_4_1000.jpg  281.259045  187.035071  327.727931  223.225547\n",
              "1    vid_4_10000.jpg   15.163531  187.035071  120.329957  236.430180\n",
              "2    vid_4_10040.jpg  239.192475  176.764801  361.968162  236.430180\n",
              "3    vid_4_10020.jpg  496.483358  172.363256  630.020260  231.539575\n",
              "4    vid_4_10060.jpg   16.630970  186.546010  132.558611  238.386422\n",
              "..               ...         ...         ...         ...         ...\n",
              "554   vid_4_9860.jpg    0.000000  198.321729   49.235251  236.223284\n",
              "555   vid_4_9880.jpg  329.876184  156.482351  536.664239  250.497895\n",
              "556   vid_4_9900.jpg    0.000000  168.295823  141.797524  239.176652\n",
              "557   vid_4_9960.jpg  487.428988  172.233646  616.917699  228.839864\n",
              "558   vid_4_9980.jpg  221.558631  182.570434  348.585579  238.192196\n",
              "\n",
              "[559 rows x 5 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT9jgVRdqOBb"
      },
      "outputs": [],
      "source": [
        "# As there is only one class \n",
        "df_basic[\"class\"] = 'car'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02tw5JcwqUwR"
      },
      "outputs": [],
      "source": [
        "df_basic['width'] = df_basic['xmax']-df_basic['xmin']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eXVbWPfqkrE"
      },
      "outputs": [],
      "source": [
        "df_basic['height'] = df_basic['ymax'] = df_basic['ymin']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LivoEXiMqqrY",
        "outputId": "9f076eac-ceaa-436f-ed1d-80055b814818"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-505c605a-78ad-4f74-97f8-7de020a86caa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>class</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vid_4_1000.jpg</td>\n",
              "      <td>281.259045</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>327.727931</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>car</td>\n",
              "      <td>46.468886</td>\n",
              "      <td>187.035071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vid_4_10000.jpg</td>\n",
              "      <td>15.163531</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>120.329957</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>car</td>\n",
              "      <td>105.166425</td>\n",
              "      <td>187.035071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vid_4_10040.jpg</td>\n",
              "      <td>239.192475</td>\n",
              "      <td>176.764801</td>\n",
              "      <td>361.968162</td>\n",
              "      <td>176.764801</td>\n",
              "      <td>car</td>\n",
              "      <td>122.775687</td>\n",
              "      <td>176.764801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vid_4_10020.jpg</td>\n",
              "      <td>496.483358</td>\n",
              "      <td>172.363256</td>\n",
              "      <td>630.020260</td>\n",
              "      <td>172.363256</td>\n",
              "      <td>car</td>\n",
              "      <td>133.536903</td>\n",
              "      <td>172.363256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vid_4_10060.jpg</td>\n",
              "      <td>16.630970</td>\n",
              "      <td>186.546010</td>\n",
              "      <td>132.558611</td>\n",
              "      <td>186.546010</td>\n",
              "      <td>car</td>\n",
              "      <td>115.927641</td>\n",
              "      <td>186.546010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>vid_4_9860.jpg</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>198.321729</td>\n",
              "      <td>49.235251</td>\n",
              "      <td>198.321729</td>\n",
              "      <td>car</td>\n",
              "      <td>49.235251</td>\n",
              "      <td>198.321729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>vid_4_9880.jpg</td>\n",
              "      <td>329.876184</td>\n",
              "      <td>156.482351</td>\n",
              "      <td>536.664239</td>\n",
              "      <td>156.482351</td>\n",
              "      <td>car</td>\n",
              "      <td>206.788055</td>\n",
              "      <td>156.482351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>vid_4_9900.jpg</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>168.295823</td>\n",
              "      <td>141.797524</td>\n",
              "      <td>168.295823</td>\n",
              "      <td>car</td>\n",
              "      <td>141.797524</td>\n",
              "      <td>168.295823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>vid_4_9960.jpg</td>\n",
              "      <td>487.428988</td>\n",
              "      <td>172.233646</td>\n",
              "      <td>616.917699</td>\n",
              "      <td>172.233646</td>\n",
              "      <td>car</td>\n",
              "      <td>129.488711</td>\n",
              "      <td>172.233646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>vid_4_9980.jpg</td>\n",
              "      <td>221.558631</td>\n",
              "      <td>182.570434</td>\n",
              "      <td>348.585579</td>\n",
              "      <td>182.570434</td>\n",
              "      <td>car</td>\n",
              "      <td>127.026948</td>\n",
              "      <td>182.570434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>559 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-505c605a-78ad-4f74-97f8-7de020a86caa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-505c605a-78ad-4f74-97f8-7de020a86caa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-505c605a-78ad-4f74-97f8-7de020a86caa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               image        xmin        ymin        xmax        ymax class  \\\n",
              "0     vid_4_1000.jpg  281.259045  187.035071  327.727931  187.035071   car   \n",
              "1    vid_4_10000.jpg   15.163531  187.035071  120.329957  187.035071   car   \n",
              "2    vid_4_10040.jpg  239.192475  176.764801  361.968162  176.764801   car   \n",
              "3    vid_4_10020.jpg  496.483358  172.363256  630.020260  172.363256   car   \n",
              "4    vid_4_10060.jpg   16.630970  186.546010  132.558611  186.546010   car   \n",
              "..               ...         ...         ...         ...         ...   ...   \n",
              "554   vid_4_9860.jpg    0.000000  198.321729   49.235251  198.321729   car   \n",
              "555   vid_4_9880.jpg  329.876184  156.482351  536.664239  156.482351   car   \n",
              "556   vid_4_9900.jpg    0.000000  168.295823  141.797524  168.295823   car   \n",
              "557   vid_4_9960.jpg  487.428988  172.233646  616.917699  172.233646   car   \n",
              "558   vid_4_9980.jpg  221.558631  182.570434  348.585579  182.570434   car   \n",
              "\n",
              "          width      height  \n",
              "0     46.468886  187.035071  \n",
              "1    105.166425  187.035071  \n",
              "2    122.775687  176.764801  \n",
              "3    133.536903  172.363256  \n",
              "4    115.927641  186.546010  \n",
              "..          ...         ...  \n",
              "554   49.235251  198.321729  \n",
              "555  206.788055  156.482351  \n",
              "556  141.797524  168.295823  \n",
              "557  129.488711  172.233646  \n",
              "558  127.026948  182.570434  \n",
              "\n",
              "[559 rows x 8 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ArJY15jLqtnp",
        "outputId": "30df2602-062b-410e-ef1f-fc402eb485a3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ceeee86a-84f8-4d0a-83ea-cb4dec1e8b3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>class</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vid_4_1000.jpg</td>\n",
              "      <td>281.259045</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>327.727931</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>car</td>\n",
              "      <td>46.468886</td>\n",
              "      <td>187.035071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vid_4_10000.jpg</td>\n",
              "      <td>15.163531</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>120.329957</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>car</td>\n",
              "      <td>105.166425</td>\n",
              "      <td>187.035071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vid_4_10040.jpg</td>\n",
              "      <td>239.192475</td>\n",
              "      <td>176.764801</td>\n",
              "      <td>361.968162</td>\n",
              "      <td>176.764801</td>\n",
              "      <td>car</td>\n",
              "      <td>122.775687</td>\n",
              "      <td>176.764801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vid_4_10020.jpg</td>\n",
              "      <td>496.483358</td>\n",
              "      <td>172.363256</td>\n",
              "      <td>630.020260</td>\n",
              "      <td>172.363256</td>\n",
              "      <td>car</td>\n",
              "      <td>133.536903</td>\n",
              "      <td>172.363256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vid_4_10060.jpg</td>\n",
              "      <td>16.630970</td>\n",
              "      <td>186.546010</td>\n",
              "      <td>132.558611</td>\n",
              "      <td>186.546010</td>\n",
              "      <td>car</td>\n",
              "      <td>115.927641</td>\n",
              "      <td>186.546010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>vid_4_9860.jpg</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>198.321729</td>\n",
              "      <td>49.235251</td>\n",
              "      <td>198.321729</td>\n",
              "      <td>car</td>\n",
              "      <td>49.235251</td>\n",
              "      <td>198.321729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>vid_4_9880.jpg</td>\n",
              "      <td>329.876184</td>\n",
              "      <td>156.482351</td>\n",
              "      <td>536.664239</td>\n",
              "      <td>156.482351</td>\n",
              "      <td>car</td>\n",
              "      <td>206.788055</td>\n",
              "      <td>156.482351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>vid_4_9900.jpg</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>168.295823</td>\n",
              "      <td>141.797524</td>\n",
              "      <td>168.295823</td>\n",
              "      <td>car</td>\n",
              "      <td>141.797524</td>\n",
              "      <td>168.295823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>vid_4_9960.jpg</td>\n",
              "      <td>487.428988</td>\n",
              "      <td>172.233646</td>\n",
              "      <td>616.917699</td>\n",
              "      <td>172.233646</td>\n",
              "      <td>car</td>\n",
              "      <td>129.488711</td>\n",
              "      <td>172.233646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>vid_4_9980.jpg</td>\n",
              "      <td>221.558631</td>\n",
              "      <td>182.570434</td>\n",
              "      <td>348.585579</td>\n",
              "      <td>182.570434</td>\n",
              "      <td>car</td>\n",
              "      <td>127.026948</td>\n",
              "      <td>182.570434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>559 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ceeee86a-84f8-4d0a-83ea-cb4dec1e8b3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ceeee86a-84f8-4d0a-83ea-cb4dec1e8b3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ceeee86a-84f8-4d0a-83ea-cb4dec1e8b3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            filename        xmin        ymin        xmax        ymax class  \\\n",
              "0     vid_4_1000.jpg  281.259045  187.035071  327.727931  187.035071   car   \n",
              "1    vid_4_10000.jpg   15.163531  187.035071  120.329957  187.035071   car   \n",
              "2    vid_4_10040.jpg  239.192475  176.764801  361.968162  176.764801   car   \n",
              "3    vid_4_10020.jpg  496.483358  172.363256  630.020260  172.363256   car   \n",
              "4    vid_4_10060.jpg   16.630970  186.546010  132.558611  186.546010   car   \n",
              "..               ...         ...         ...         ...         ...   ...   \n",
              "554   vid_4_9860.jpg    0.000000  198.321729   49.235251  198.321729   car   \n",
              "555   vid_4_9880.jpg  329.876184  156.482351  536.664239  156.482351   car   \n",
              "556   vid_4_9900.jpg    0.000000  168.295823  141.797524  168.295823   car   \n",
              "557   vid_4_9960.jpg  487.428988  172.233646  616.917699  172.233646   car   \n",
              "558   vid_4_9980.jpg  221.558631  182.570434  348.585579  182.570434   car   \n",
              "\n",
              "          width      height  \n",
              "0     46.468886  187.035071  \n",
              "1    105.166425  187.035071  \n",
              "2    122.775687  176.764801  \n",
              "3    133.536903  172.363256  \n",
              "4    115.927641  186.546010  \n",
              "..          ...         ...  \n",
              "554   49.235251  198.321729  \n",
              "555  206.788055  156.482351  \n",
              "556  141.797524  168.295823  \n",
              "557  129.488711  172.233646  \n",
              "558  127.026948  182.570434  \n",
              "\n",
              "[559 rows x 8 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_basic.rename(columns={'image':'filename'}, inplace = True)\n",
        "df_basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwwZxJjNq8gi"
      },
      "outputs": [],
      "source": [
        "df_basic = df_basic[['filename','width','height','class','xmin','ymin','xmax','ymax']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGP76h5gwLBv"
      },
      "source": [
        "## Splitting the total record in test and train data. \n",
        "\n",
        "We have 559 rows and we will take 459 as training images and remaining 100 as test images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CDOfMjynwKkD",
        "outputId": "950f86e1-7bbb-446c-d5df-19b3d1eee562"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-63e29aae-cc93-4c81-8557-cd6108d29fea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>vid_4_6440.jpg</td>\n",
              "      <td>77.285094</td>\n",
              "      <td>192.414736</td>\n",
              "      <td>car</td>\n",
              "      <td>56.740955</td>\n",
              "      <td>192.414736</td>\n",
              "      <td>134.026049</td>\n",
              "      <td>192.414736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>vid_4_6440.jpg</td>\n",
              "      <td>112.992764</td>\n",
              "      <td>180.188224</td>\n",
              "      <td>car</td>\n",
              "      <td>415.774240</td>\n",
              "      <td>180.188224</td>\n",
              "      <td>528.767004</td>\n",
              "      <td>180.188224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461</th>\n",
              "      <td>vid_4_6440.jpg</td>\n",
              "      <td>75.328509</td>\n",
              "      <td>178.721043</td>\n",
              "      <td>car</td>\n",
              "      <td>534.147612</td>\n",
              "      <td>178.721043</td>\n",
              "      <td>609.476122</td>\n",
              "      <td>178.721043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>vid_4_6460.jpg</td>\n",
              "      <td>105.655572</td>\n",
              "      <td>191.925676</td>\n",
              "      <td>car</td>\n",
              "      <td>184.408104</td>\n",
              "      <td>191.925676</td>\n",
              "      <td>290.063676</td>\n",
              "      <td>191.925676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>vid_4_6460.jpg</td>\n",
              "      <td>88.046310</td>\n",
              "      <td>182.144466</td>\n",
              "      <td>car</td>\n",
              "      <td>357.076700</td>\n",
              "      <td>182.144466</td>\n",
              "      <td>445.123010</td>\n",
              "      <td>182.144466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>vid_4_9860.jpg</td>\n",
              "      <td>49.235251</td>\n",
              "      <td>198.321729</td>\n",
              "      <td>car</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>198.321729</td>\n",
              "      <td>49.235251</td>\n",
              "      <td>198.321729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>vid_4_9880.jpg</td>\n",
              "      <td>206.788055</td>\n",
              "      <td>156.482351</td>\n",
              "      <td>car</td>\n",
              "      <td>329.876184</td>\n",
              "      <td>156.482351</td>\n",
              "      <td>536.664239</td>\n",
              "      <td>156.482351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>vid_4_9900.jpg</td>\n",
              "      <td>141.797524</td>\n",
              "      <td>168.295823</td>\n",
              "      <td>car</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>168.295823</td>\n",
              "      <td>141.797524</td>\n",
              "      <td>168.295823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>vid_4_9960.jpg</td>\n",
              "      <td>129.488711</td>\n",
              "      <td>172.233646</td>\n",
              "      <td>car</td>\n",
              "      <td>487.428988</td>\n",
              "      <td>172.233646</td>\n",
              "      <td>616.917699</td>\n",
              "      <td>172.233646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>vid_4_9980.jpg</td>\n",
              "      <td>127.026948</td>\n",
              "      <td>182.570434</td>\n",
              "      <td>car</td>\n",
              "      <td>221.558631</td>\n",
              "      <td>182.570434</td>\n",
              "      <td>348.585579</td>\n",
              "      <td>182.570434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63e29aae-cc93-4c81-8557-cd6108d29fea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-63e29aae-cc93-4c81-8557-cd6108d29fea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-63e29aae-cc93-4c81-8557-cd6108d29fea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           filename       width      height class        xmin        ymin  \\\n",
              "459  vid_4_6440.jpg   77.285094  192.414736   car   56.740955  192.414736   \n",
              "460  vid_4_6440.jpg  112.992764  180.188224   car  415.774240  180.188224   \n",
              "461  vid_4_6440.jpg   75.328509  178.721043   car  534.147612  178.721043   \n",
              "462  vid_4_6460.jpg  105.655572  191.925676   car  184.408104  191.925676   \n",
              "463  vid_4_6460.jpg   88.046310  182.144466   car  357.076700  182.144466   \n",
              "..              ...         ...         ...   ...         ...         ...   \n",
              "554  vid_4_9860.jpg   49.235251  198.321729   car    0.000000  198.321729   \n",
              "555  vid_4_9880.jpg  206.788055  156.482351   car  329.876184  156.482351   \n",
              "556  vid_4_9900.jpg  141.797524  168.295823   car    0.000000  168.295823   \n",
              "557  vid_4_9960.jpg  129.488711  172.233646   car  487.428988  172.233646   \n",
              "558  vid_4_9980.jpg  127.026948  182.570434   car  221.558631  182.570434   \n",
              "\n",
              "           xmax        ymax  \n",
              "459  134.026049  192.414736  \n",
              "460  528.767004  180.188224  \n",
              "461  609.476122  178.721043  \n",
              "462  290.063676  191.925676  \n",
              "463  445.123010  182.144466  \n",
              "..          ...         ...  \n",
              "554   49.235251  198.321729  \n",
              "555  536.664239  156.482351  \n",
              "556  141.797524  168.295823  \n",
              "557  616.917699  172.233646  \n",
              "558  348.585579  182.570434  \n",
              "\n",
              "[100 rows x 8 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_basic_test = df_basic[459::]\n",
        "df_basic_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "WU9mnHCIxqQ1",
        "outputId": "a577d4e7-507d-499e-f184-abc3d7829856"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-faeba40f-4268-40e1-9d31-775a56aa26de\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>class</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vid_4_1000.jpg</td>\n",
              "      <td>46.468886</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>car</td>\n",
              "      <td>281.259045</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>327.727931</td>\n",
              "      <td>187.035071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vid_4_10000.jpg</td>\n",
              "      <td>105.166425</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>car</td>\n",
              "      <td>15.163531</td>\n",
              "      <td>187.035071</td>\n",
              "      <td>120.329957</td>\n",
              "      <td>187.035071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vid_4_10040.jpg</td>\n",
              "      <td>122.775687</td>\n",
              "      <td>176.764801</td>\n",
              "      <td>car</td>\n",
              "      <td>239.192475</td>\n",
              "      <td>176.764801</td>\n",
              "      <td>361.968162</td>\n",
              "      <td>176.764801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vid_4_10020.jpg</td>\n",
              "      <td>133.536903</td>\n",
              "      <td>172.363256</td>\n",
              "      <td>car</td>\n",
              "      <td>496.483358</td>\n",
              "      <td>172.363256</td>\n",
              "      <td>630.020260</td>\n",
              "      <td>172.363256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vid_4_10060.jpg</td>\n",
              "      <td>115.927641</td>\n",
              "      <td>186.546010</td>\n",
              "      <td>car</td>\n",
              "      <td>16.630970</td>\n",
              "      <td>186.546010</td>\n",
              "      <td>132.558611</td>\n",
              "      <td>186.546010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>vid_4_6400.jpg</td>\n",
              "      <td>126.688857</td>\n",
              "      <td>184.100708</td>\n",
              "      <td>car</td>\n",
              "      <td>159.461650</td>\n",
              "      <td>184.100708</td>\n",
              "      <td>286.150507</td>\n",
              "      <td>184.100708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>vid_4_6400.jpg</td>\n",
              "      <td>99.296672</td>\n",
              "      <td>190.458494</td>\n",
              "      <td>car</td>\n",
              "      <td>346.315485</td>\n",
              "      <td>190.458494</td>\n",
              "      <td>445.612156</td>\n",
              "      <td>190.458494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>vid_4_6420.jpg</td>\n",
              "      <td>98.807525</td>\n",
              "      <td>184.100708</td>\n",
              "      <td>car</td>\n",
              "      <td>6.358900</td>\n",
              "      <td>184.100708</td>\n",
              "      <td>105.166426</td>\n",
              "      <td>184.100708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>vid_4_6420.jpg</td>\n",
              "      <td>77.285094</td>\n",
              "      <td>188.502252</td>\n",
              "      <td>car</td>\n",
              "      <td>191.256150</td>\n",
              "      <td>188.502252</td>\n",
              "      <td>268.541245</td>\n",
              "      <td>188.502252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>vid_4_6420.jpg</td>\n",
              "      <td>63.589001</td>\n",
              "      <td>179.699163</td>\n",
              "      <td>car</td>\n",
              "      <td>612.410999</td>\n",
              "      <td>179.699163</td>\n",
              "      <td>676.000000</td>\n",
              "      <td>179.699163</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>459 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-faeba40f-4268-40e1-9d31-775a56aa26de')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-faeba40f-4268-40e1-9d31-775a56aa26de button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-faeba40f-4268-40e1-9d31-775a56aa26de');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            filename       width      height class        xmin        ymin  \\\n",
              "0     vid_4_1000.jpg   46.468886  187.035071   car  281.259045  187.035071   \n",
              "1    vid_4_10000.jpg  105.166425  187.035071   car   15.163531  187.035071   \n",
              "2    vid_4_10040.jpg  122.775687  176.764801   car  239.192475  176.764801   \n",
              "3    vid_4_10020.jpg  133.536903  172.363256   car  496.483358  172.363256   \n",
              "4    vid_4_10060.jpg  115.927641  186.546010   car   16.630970  186.546010   \n",
              "..               ...         ...         ...   ...         ...         ...   \n",
              "454   vid_4_6400.jpg  126.688857  184.100708   car  159.461650  184.100708   \n",
              "455   vid_4_6400.jpg   99.296672  190.458494   car  346.315485  190.458494   \n",
              "456   vid_4_6420.jpg   98.807525  184.100708   car    6.358900  184.100708   \n",
              "457   vid_4_6420.jpg   77.285094  188.502252   car  191.256150  188.502252   \n",
              "458   vid_4_6420.jpg   63.589001  179.699163   car  612.410999  179.699163   \n",
              "\n",
              "           xmax        ymax  \n",
              "0    327.727931  187.035071  \n",
              "1    120.329957  187.035071  \n",
              "2    361.968162  176.764801  \n",
              "3    630.020260  172.363256  \n",
              "4    132.558611  186.546010  \n",
              "..          ...         ...  \n",
              "454  286.150507  184.100708  \n",
              "455  445.612156  190.458494  \n",
              "456  105.166426  184.100708  \n",
              "457  268.541245  188.502252  \n",
              "458  676.000000  179.699163  \n",
              "\n",
              "[459 rows x 8 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_basic_train = df_basic[0:459]\n",
        "df_basic_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyAwH3Rkx3kA"
      },
      "outputs": [],
      "source": [
        "df_basic_train.to_csv(r\"/content/data/train.csv\", index = False)\n",
        "df_basic_test.to_csv(r\"/content/data/test.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge7lRnUjjeQG",
        "outputId": "2e31c192-e2ba-4b33-809c-4891ab4cdad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-10-02 08:21:26.525712: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-10-02 08:21:27.207831: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-02 08:21:27.207935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-02 08:21:27.207954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Successfully created the TFRecords: /content/models/research/train.record\n"
          ]
        }
      ],
      "source": [
        "# We need to put the python file called generate_tfrecord.py in our research folder. \n",
        "# For the python file : https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py\n",
        "!python generate_tfrecord.py --csv_input=\"/content/data/train.csv\" --image_dir=\"/content/data/training_images\" --output_path=\"train.record\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBLTBTTfyKjB",
        "outputId": "b562fc02-35e6-41d1-ad61-b338990e8349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-10-02 08:21:31.619260: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-10-02 08:21:32.291558: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-02 08:21:32.291663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-02 08:21:32.291682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Successfully created the TFRecords: /content/models/research/test.record\n"
          ]
        }
      ],
      "source": [
        "# We need to put the python file called generate_tfrecord.py in our research folder. \n",
        "# For the python file : https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py\n",
        "!python generate_tfrecord.py --csv_input=\"/content/data/test.csv\" --image_dir=\"/content/data/training_images\" --output_path=\"test.record\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYxshSQiv9jz"
      },
      "source": [
        "## Lets train the model now "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhOSXPlEryic"
      },
      "outputs": [],
      "source": [
        "# Before we start training lets edit the config file for the following things \n",
        "# change the num_classes to 1 \n",
        "# change the fine tune checkpoint path to retinanet50/checkpoint/ckpt-0\n",
        "# change num_steps to 500 reducing number\n",
        "# change finetune checkpoint type to detection \n",
        "# change the labelmap path to labelmap.pbtxt\n",
        "# train input reader path to train.record\n",
        "# eval input reader path to test.record\n",
        "# change the batch size as per your GPU capacity \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_NaFfhzq1SCW",
        "outputId": "386aa65b-a583-473a-8f29-44cbb41d6500"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jed0o2LD4Bcl",
        "outputId": "4049afef-ee1d-4d4e-ca67-f0bdf06ef371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 18 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 3,139 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 7s (63.3 MB/s)\n",
            "(Reading database ... 159447 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 159425 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"
          ]
        }
      ],
      "source": [
        "# Installing the latest CUDA version \n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNFwO_PI0J3Z",
        "outputId": "5072f306-7dd6-4603-c3d5-96b3a74c763d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-10-02 08:57:39.508660: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-10-02 08:57:41.866526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-02 08:57:41.867274: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-02 08:57:41.867301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2022-10-02 08:57:50.479077: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1002 08:57:50.553113 140668201981824 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I1002 08:57:50.558499 140668201981824 config_util.py:552] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I1002 08:57:50.558655 140668201981824 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W1002 08:57:50.585949 140668201981824 deprecation.py:356] From /content/models/research/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['train.record']\n",
            "I1002 08:57:50.602979 140668201981824 dataset_builder.py:162] Reading unweighted datasets: ['train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['train.record']\n",
            "I1002 08:57:50.606873 140668201981824 dataset_builder.py:79] Reading record datasets for input file: ['train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I1002 08:57:50.607001 140668201981824 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1002 08:57:50.607089 140668201981824 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W1002 08:57:50.617692 140668201981824 deprecation.py:356] From /content/models/research/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1002 08:57:50.642842 140668201981824 deprecation.py:356] From /content/models/research/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W1002 08:57:57.256277 140668201981824 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W1002 08:58:00.061741 140668201981824 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W1002 08:58:01.644705 140668201981824 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  \"`tf.keras.backend.set_learning_phase` is deprecated and \"\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.899329 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.902373 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.905093 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.906157 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.908835 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.909882 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.913609 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.914634 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.916288 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1002 08:58:35.917330 140668201981824 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W1002 08:58:37.338170 140663341434624 deprecation.py:560] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "INFO:tensorflow:Step 100 per-step time 1.027s\n",
            "I1002 09:00:19.689418 140668201981824 model_lib_v2.py:707] Step 100 per-step time 1.027s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 2.718771e-07,\n",
            " 'Loss/localization_loss': 0.0,\n",
            " 'Loss/regularization_loss': 0.24193215,\n",
            " 'Loss/total_loss': 0.24193242,\n",
            " 'learning_rate': 0.014666351}\n",
            "I1002 09:00:19.689754 140668201981824 model_lib_v2.py:708] {'Loss/classification_loss': 2.718771e-07,\n",
            " 'Loss/localization_loss': 0.0,\n",
            " 'Loss/regularization_loss': 0.24193215,\n",
            " 'Loss/total_loss': 0.24193242,\n",
            " 'learning_rate': 0.014666351}\n"
          ]
        }
      ],
      "source": [
        "# Executing the model using the pipeline configs \n",
        "# put the model_main_tf2.py file outside in object detection folder\n",
        "# because the path is not detected sometimes \n",
        "!python model_main_tf2.py --model_dir=training --pipeline_config_path=training/pipeline.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPL1kwZS7q7Z"
      },
      "source": [
        "# Exporting the trained model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl9yMPIm1UTr",
        "outputId": "43e557d8-73a6-4f37-b6a6-0715233865ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-10-02 09:10:59.411485: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-10-02 09:11:00.143780: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-02 09:11:00.143892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-02 09:11:00.143912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2022-10-02 09:11:02.894733: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.map_fn(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "W1002 09:11:03.153494 140322570069888 deprecation.py:628] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:458: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.map_fn(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.map_fn(fn, elems))\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f9ed01268d0>, because it is not built.\n",
            "W1002 09:11:21.989307 140322570069888 save_impl.py:68] Skipping full serialization of Keras layer <object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch object at 0x7f9ed01268d0>, because it is not built.\n",
            "W1002 09:11:40.223609 140322570069888 save.py:274] Found untraced functions such as WeightSharedConvolutionalBoxPredictor_layer_call_fn, WeightSharedConvolutionalBoxPredictor_layer_call_and_return_conditional_losses, WeightSharedConvolutionalBoxHead_layer_call_fn, WeightSharedConvolutionalBoxHead_layer_call_and_return_conditional_losses, WeightSharedConvolutionalClassHead_layer_call_fn while saving (showing 5 of 278). These functions will not be directly callable after loading.\n",
            "INFO:tensorflow:Assets written to: car_detection/model/saved_model/assets\n",
            "I1002 09:11:46.672368 140322570069888 builder_impl.py:780] Assets written to: car_detection/model/saved_model/assets\n",
            "INFO:tensorflow:Writing pipeline config file to car_detection/model/pipeline.config\n",
            "I1002 09:11:47.515492 140322570069888 config_util.py:254] Writing pipeline config file to car_detection/model/pipeline.config\n"
          ]
        }
      ],
      "source": [
        "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path training/pipeline.config --trained_checkpoint_dir training --output_directory car_detection/model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "674AA6Gj-J6x"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_QEbred8q5D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLv0bPp7-N1H"
      },
      "outputs": [],
      "source": [
        "# Importing object detection \n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi12NJDu-QKI"
      },
      "outputs": [],
      "source": [
        "# patch tf1 into `utils.ops`\n",
        "utils_ops.tf = tf.compat.v1\n",
        "\n",
        "# Patch the location of gfile\n",
        "tf.gfile = tf.io.gfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzIAeKni-c4Q"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "def load_custom_model(model_name):\n",
        "  model_file = model_name\n",
        "  model_dir = pathlib.Path(model_file)/\"saved_model\"\n",
        "  model = tf.saved_model.load(str(model_dir))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as7Ws-Jh-wyj",
        "outputId": "72526671-8c35-48b9-cf5e-8d2c75742ed5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PosixPath('/content/data/testing_images/vid_5_25100.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_25120.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_25140.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_25160.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_25180.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_25200.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_25220.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_25240.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_25260.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26320.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26400.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26420.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26560.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26580.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26600.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26620.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26640.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26660.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26680.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26700.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26720.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26740.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26760.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26780.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26800.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26820.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26840.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26860.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26880.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26900.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26920.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26940.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26960.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_26980.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27240.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27260.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27280.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27300.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27320.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27360.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27380.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27400.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27420.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27440.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27460.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27480.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27500.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27520.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27540.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27560.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27580.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27600.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27620.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27640.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27660.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27680.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27700.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27720.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27740.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27760.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27780.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27800.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27820.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27840.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27860.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27880.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27900.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27920.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27940.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27960.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_27980.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28000.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28020.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28040.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28060.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28080.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28180.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28260.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28320.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28340.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28360.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28380.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28420.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28440.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28460.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28480.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28500.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28520.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28540.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28560.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28580.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28600.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28620.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28640.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28660.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28680.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_28700.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29000.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29020.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29040.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29060.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29080.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29100.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29400.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29420.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29440.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29460.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29480.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29500.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29520.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29540.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29560.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29580.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29600.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29620.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29640.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29720.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29740.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29760.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29820.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29840.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29860.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29880.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29900.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_29980.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30000.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30020.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30040.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30120.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30140.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30160.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30180.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30640.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30660.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30680.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30700.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30720.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30740.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30760.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30820.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30840.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30860.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30920.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_30940.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31020.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31040.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31060.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31080.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31100.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31120.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31140.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31160.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31180.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31200.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31260.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31280.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31300.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31360.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31380.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31400.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31420.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31480.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31500.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31520.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31560.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31600.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31620.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31640.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31660.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31680.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31700.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_31720.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_400.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_420.jpg'),\n",
              " PosixPath('/content/data/testing_images/vid_5_440.jpg')]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = 'training/labelmap.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('/content/data/testing_images')\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
        "TEST_IMAGE_PATHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWupY2_N_Jwz"
      },
      "outputs": [],
      "source": [
        "model_name = '/content/models/research/car_detection/model'\n",
        "detection_model = load_custom_model(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRKoPvbh_d4-",
        "outputId": "71cb5161-c09f-4df6-ecd0-5f9b1058c9cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'input_tensor:0' shape=(1, None, None, 3) dtype=uint8>, <tf.Tensor 'unknown:0' shape=() dtype=resource>, <tf.Tensor 'unknown_0:0' shape=() dtype=resource>, <tf.Tensor 'unknown_1:0' shape=() dtype=resource>, <tf.Tensor 'unknown_2:0' shape=() dtype=resource>, <tf.Tensor 'unknown_3:0' shape=() dtype=resource>, <tf.Tensor 'unknown_4:0' shape=() dtype=resource>, <tf.Tensor 'unknown_5:0' shape=() dtype=resource>, <tf.Tensor 'unknown_6:0' shape=() dtype=resource>, <tf.Tensor 'unknown_7:0' shape=() dtype=resource>, <tf.Tensor 'unknown_8:0' shape=() dtype=resource>, <tf.Tensor 'unknown_9:0' shape=() dtype=resource>, <tf.Tensor 'unknown_10:0' shape=() dtype=resource>, <tf.Tensor 'unknown_11:0' shape=() dtype=resource>, <tf.Tensor 'unknown_12:0' shape=() dtype=resource>, <tf.Tensor 'unknown_13:0' shape=() dtype=resource>, <tf.Tensor 'unknown_14:0' shape=() dtype=resource>, <tf.Tensor 'unknown_15:0' shape=() dtype=resource>, <tf.Tensor 'unknown_16:0' shape=() dtype=resource>, <tf.Tensor 'unknown_17:0' shape=() dtype=resource>, <tf.Tensor 'unknown_18:0' shape=() dtype=resource>, <tf.Tensor 'unknown_19:0' shape=() dtype=resource>, <tf.Tensor 'unknown_20:0' shape=() dtype=resource>, <tf.Tensor 'unknown_21:0' shape=() dtype=resource>, <tf.Tensor 'unknown_22:0' shape=() dtype=resource>, <tf.Tensor 'unknown_23:0' shape=() dtype=resource>, <tf.Tensor 'unknown_24:0' shape=() dtype=resource>, <tf.Tensor 'unknown_25:0' shape=() dtype=resource>, <tf.Tensor 'unknown_26:0' shape=() dtype=resource>, <tf.Tensor 'unknown_27:0' shape=() dtype=resource>, <tf.Tensor 'unknown_28:0' shape=() dtype=resource>, <tf.Tensor 'unknown_29:0' shape=() dtype=resource>, <tf.Tensor 'unknown_30:0' shape=() dtype=resource>, <tf.Tensor 'unknown_31:0' shape=() dtype=resource>, <tf.Tensor 'unknown_32:0' shape=() dtype=resource>, <tf.Tensor 'unknown_33:0' shape=() dtype=resource>, <tf.Tensor 'unknown_34:0' shape=() dtype=resource>, <tf.Tensor 'unknown_35:0' shape=() dtype=resource>, <tf.Tensor 'unknown_36:0' shape=() dtype=resource>, <tf.Tensor 'unknown_37:0' shape=() dtype=resource>, <tf.Tensor 'unknown_38:0' shape=() dtype=resource>, <tf.Tensor 'unknown_39:0' shape=() dtype=resource>, <tf.Tensor 'unknown_40:0' shape=() dtype=resource>, <tf.Tensor 'unknown_41:0' shape=() dtype=resource>, <tf.Tensor 'unknown_42:0' shape=() dtype=resource>, <tf.Tensor 'unknown_43:0' shape=() dtype=resource>, <tf.Tensor 'unknown_44:0' shape=() dtype=resource>, <tf.Tensor 'unknown_45:0' shape=() dtype=resource>, <tf.Tensor 'unknown_46:0' shape=() dtype=resource>, <tf.Tensor 'unknown_47:0' shape=() dtype=resource>, <tf.Tensor 'unknown_48:0' shape=() dtype=resource>, <tf.Tensor 'unknown_49:0' shape=() dtype=resource>, <tf.Tensor 'unknown_50:0' shape=() dtype=resource>, <tf.Tensor 'unknown_51:0' shape=() dtype=resource>, <tf.Tensor 'unknown_52:0' shape=() dtype=resource>, <tf.Tensor 'unknown_53:0' shape=() dtype=resource>, <tf.Tensor 'unknown_54:0' shape=() dtype=resource>, <tf.Tensor 'unknown_55:0' shape=() dtype=resource>, <tf.Tensor 'unknown_56:0' shape=() dtype=resource>, <tf.Tensor 'unknown_57:0' shape=() dtype=resource>, <tf.Tensor 'unknown_58:0' shape=() dtype=resource>, <tf.Tensor 'unknown_59:0' shape=() dtype=resource>, <tf.Tensor 'unknown_60:0' shape=() dtype=resource>, <tf.Tensor 'unknown_61:0' shape=() dtype=resource>, <tf.Tensor 'unknown_62:0' shape=() dtype=resource>, <tf.Tensor 'unknown_63:0' shape=() dtype=resource>, <tf.Tensor 'unknown_64:0' shape=() dtype=resource>, <tf.Tensor 'unknown_65:0' shape=() dtype=resource>, <tf.Tensor 'unknown_66:0' shape=() dtype=resource>, <tf.Tensor 'unknown_67:0' shape=() dtype=resource>, <tf.Tensor 'unknown_68:0' shape=() dtype=resource>, <tf.Tensor 'unknown_69:0' shape=() dtype=resource>, <tf.Tensor 'unknown_70:0' shape=() dtype=resource>, <tf.Tensor 'unknown_71:0' shape=() dtype=resource>, <tf.Tensor 'unknown_72:0' shape=() dtype=resource>, <tf.Tensor 'unknown_73:0' shape=() dtype=resource>, <tf.Tensor 'unknown_74:0' shape=() dtype=resource>, <tf.Tensor 'unknown_75:0' shape=() dtype=resource>, <tf.Tensor 'unknown_76:0' shape=() dtype=resource>, <tf.Tensor 'unknown_77:0' shape=() dtype=resource>, <tf.Tensor 'unknown_78:0' shape=() dtype=resource>, <tf.Tensor 'unknown_79:0' shape=() dtype=resource>, <tf.Tensor 'unknown_80:0' shape=() dtype=resource>, <tf.Tensor 'unknown_81:0' shape=() dtype=resource>, <tf.Tensor 'unknown_82:0' shape=() dtype=resource>, <tf.Tensor 'unknown_83:0' shape=() dtype=resource>, <tf.Tensor 'unknown_84:0' shape=() dtype=resource>, <tf.Tensor 'unknown_85:0' shape=() dtype=resource>, <tf.Tensor 'unknown_86:0' shape=() dtype=resource>, <tf.Tensor 'unknown_87:0' shape=() dtype=resource>, <tf.Tensor 'unknown_88:0' shape=() dtype=resource>, <tf.Tensor 'unknown_89:0' shape=() dtype=resource>, <tf.Tensor 'unknown_90:0' shape=() dtype=resource>, <tf.Tensor 'unknown_91:0' shape=() dtype=resource>, <tf.Tensor 'unknown_92:0' shape=() dtype=resource>, <tf.Tensor 'unknown_93:0' shape=() dtype=resource>, <tf.Tensor 'unknown_94:0' shape=() dtype=resource>, <tf.Tensor 'unknown_95:0' shape=() dtype=resource>, <tf.Tensor 'unknown_96:0' shape=() dtype=resource>, <tf.Tensor 'unknown_97:0' shape=() dtype=resource>, <tf.Tensor 'unknown_98:0' shape=() dtype=resource>, <tf.Tensor 'unknown_99:0' shape=() dtype=resource>, <tf.Tensor 'unknown_100:0' shape=() dtype=resource>, <tf.Tensor 'unknown_101:0' shape=() dtype=resource>, <tf.Tensor 'unknown_102:0' shape=() dtype=resource>, <tf.Tensor 'unknown_103:0' shape=() dtype=resource>, <tf.Tensor 'unknown_104:0' shape=() dtype=resource>, <tf.Tensor 'unknown_105:0' shape=() dtype=resource>, <tf.Tensor 'unknown_106:0' shape=() dtype=resource>, <tf.Tensor 'unknown_107:0' shape=() dtype=resource>, <tf.Tensor 'unknown_108:0' shape=() dtype=resource>, <tf.Tensor 'unknown_109:0' shape=() dtype=resource>, <tf.Tensor 'unknown_110:0' shape=() dtype=resource>, <tf.Tensor 'unknown_111:0' shape=() dtype=resource>, <tf.Tensor 'unknown_112:0' shape=() dtype=resource>, <tf.Tensor 'unknown_113:0' shape=() dtype=resource>, <tf.Tensor 'unknown_114:0' shape=() dtype=resource>, <tf.Tensor 'unknown_115:0' shape=() dtype=resource>, <tf.Tensor 'unknown_116:0' shape=() dtype=resource>, <tf.Tensor 'unknown_117:0' shape=() dtype=resource>, <tf.Tensor 'unknown_118:0' shape=() dtype=resource>, <tf.Tensor 'unknown_119:0' shape=() dtype=resource>, <tf.Tensor 'unknown_120:0' shape=() dtype=resource>, <tf.Tensor 'unknown_121:0' shape=() dtype=resource>, <tf.Tensor 'unknown_122:0' shape=() dtype=resource>, <tf.Tensor 'unknown_123:0' shape=() dtype=resource>, <tf.Tensor 'unknown_124:0' shape=() dtype=resource>, <tf.Tensor 'unknown_125:0' shape=() dtype=resource>, <tf.Tensor 'unknown_126:0' shape=() dtype=resource>, <tf.Tensor 'unknown_127:0' shape=() dtype=resource>, <tf.Tensor 'unknown_128:0' shape=() dtype=resource>, <tf.Tensor 'unknown_129:0' shape=() dtype=resource>, <tf.Tensor 'unknown_130:0' shape=() dtype=resource>, <tf.Tensor 'unknown_131:0' shape=() dtype=resource>, <tf.Tensor 'unknown_132:0' shape=() dtype=resource>, <tf.Tensor 'unknown_133:0' shape=() dtype=resource>, <tf.Tensor 'unknown_134:0' shape=() dtype=resource>, <tf.Tensor 'unknown_135:0' shape=() dtype=resource>, <tf.Tensor 'unknown_136:0' shape=() dtype=resource>, <tf.Tensor 'unknown_137:0' shape=() dtype=resource>, <tf.Tensor 'unknown_138:0' shape=() dtype=resource>, <tf.Tensor 'unknown_139:0' shape=() dtype=resource>, <tf.Tensor 'unknown_140:0' shape=() dtype=resource>, <tf.Tensor 'unknown_141:0' shape=() dtype=resource>, <tf.Tensor 'unknown_142:0' shape=() dtype=resource>, <tf.Tensor 'unknown_143:0' shape=() dtype=resource>, <tf.Tensor 'unknown_144:0' shape=() dtype=resource>, <tf.Tensor 'unknown_145:0' shape=() dtype=resource>, <tf.Tensor 'unknown_146:0' shape=() dtype=resource>, <tf.Tensor 'unknown_147:0' shape=() dtype=resource>, <tf.Tensor 'unknown_148:0' shape=() dtype=resource>, <tf.Tensor 'unknown_149:0' shape=() dtype=resource>, <tf.Tensor 'unknown_150:0' shape=() dtype=resource>, <tf.Tensor 'unknown_151:0' shape=() dtype=resource>, <tf.Tensor 'unknown_152:0' shape=() dtype=resource>, <tf.Tensor 'unknown_153:0' shape=() dtype=resource>, <tf.Tensor 'unknown_154:0' shape=() dtype=resource>, <tf.Tensor 'unknown_155:0' shape=() dtype=resource>, <tf.Tensor 'unknown_156:0' shape=() dtype=resource>, <tf.Tensor 'unknown_157:0' shape=() dtype=resource>, <tf.Tensor 'unknown_158:0' shape=() dtype=resource>, <tf.Tensor 'unknown_159:0' shape=() dtype=resource>, <tf.Tensor 'unknown_160:0' shape=() dtype=resource>, <tf.Tensor 'unknown_161:0' shape=() dtype=resource>, <tf.Tensor 'unknown_162:0' shape=() dtype=resource>, <tf.Tensor 'unknown_163:0' shape=() dtype=resource>, <tf.Tensor 'unknown_164:0' shape=() dtype=resource>, <tf.Tensor 'unknown_165:0' shape=() dtype=resource>, <tf.Tensor 'unknown_166:0' shape=() dtype=resource>, <tf.Tensor 'unknown_167:0' shape=() dtype=resource>, <tf.Tensor 'unknown_168:0' shape=() dtype=resource>, <tf.Tensor 'unknown_169:0' shape=() dtype=resource>, <tf.Tensor 'unknown_170:0' shape=() dtype=resource>, <tf.Tensor 'unknown_171:0' shape=() dtype=resource>, <tf.Tensor 'unknown_172:0' shape=() dtype=resource>, <tf.Tensor 'unknown_173:0' shape=() dtype=resource>, <tf.Tensor 'unknown_174:0' shape=() dtype=resource>, <tf.Tensor 'unknown_175:0' shape=() dtype=resource>, <tf.Tensor 'unknown_176:0' shape=() dtype=resource>, <tf.Tensor 'unknown_177:0' shape=() dtype=resource>, <tf.Tensor 'unknown_178:0' shape=() dtype=resource>, <tf.Tensor 'unknown_179:0' shape=() dtype=resource>, <tf.Tensor 'unknown_180:0' shape=() dtype=resource>, <tf.Tensor 'unknown_181:0' shape=() dtype=resource>, <tf.Tensor 'unknown_182:0' shape=() dtype=resource>, <tf.Tensor 'unknown_183:0' shape=() dtype=resource>, <tf.Tensor 'unknown_184:0' shape=() dtype=resource>, <tf.Tensor 'unknown_185:0' shape=() dtype=resource>, <tf.Tensor 'unknown_186:0' shape=() dtype=resource>, <tf.Tensor 'unknown_187:0' shape=() dtype=resource>, <tf.Tensor 'unknown_188:0' shape=() dtype=resource>, <tf.Tensor 'unknown_189:0' shape=() dtype=resource>, <tf.Tensor 'unknown_190:0' shape=() dtype=resource>, <tf.Tensor 'unknown_191:0' shape=() dtype=resource>, <tf.Tensor 'unknown_192:0' shape=() dtype=resource>, <tf.Tensor 'unknown_193:0' shape=() dtype=resource>, <tf.Tensor 'unknown_194:0' shape=() dtype=resource>, <tf.Tensor 'unknown_195:0' shape=() dtype=resource>, <tf.Tensor 'unknown_196:0' shape=() dtype=resource>, <tf.Tensor 'unknown_197:0' shape=() dtype=resource>, <tf.Tensor 'unknown_198:0' shape=() dtype=resource>, <tf.Tensor 'unknown_199:0' shape=() dtype=resource>, <tf.Tensor 'unknown_200:0' shape=() dtype=resource>, <tf.Tensor 'unknown_201:0' shape=() dtype=resource>, <tf.Tensor 'unknown_202:0' shape=() dtype=resource>, <tf.Tensor 'unknown_203:0' shape=() dtype=resource>, <tf.Tensor 'unknown_204:0' shape=() dtype=resource>, <tf.Tensor 'unknown_205:0' shape=() dtype=resource>, <tf.Tensor 'unknown_206:0' shape=() dtype=resource>, <tf.Tensor 'unknown_207:0' shape=() dtype=resource>, <tf.Tensor 'unknown_208:0' shape=() dtype=resource>, <tf.Tensor 'unknown_209:0' shape=() dtype=resource>, <tf.Tensor 'unknown_210:0' shape=() dtype=resource>, <tf.Tensor 'unknown_211:0' shape=() dtype=resource>, <tf.Tensor 'unknown_212:0' shape=() dtype=resource>, <tf.Tensor 'unknown_213:0' shape=() dtype=resource>, <tf.Tensor 'unknown_214:0' shape=() dtype=resource>, <tf.Tensor 'unknown_215:0' shape=() dtype=resource>, <tf.Tensor 'unknown_216:0' shape=() dtype=resource>, <tf.Tensor 'unknown_217:0' shape=() dtype=resource>, <tf.Tensor 'unknown_218:0' shape=() dtype=resource>, <tf.Tensor 'unknown_219:0' shape=() dtype=resource>, <tf.Tensor 'unknown_220:0' shape=() dtype=resource>, <tf.Tensor 'unknown_221:0' shape=() dtype=resource>, <tf.Tensor 'unknown_222:0' shape=() dtype=resource>, <tf.Tensor 'unknown_223:0' shape=() dtype=resource>, <tf.Tensor 'unknown_224:0' shape=() dtype=resource>, <tf.Tensor 'unknown_225:0' shape=() dtype=resource>, <tf.Tensor 'unknown_226:0' shape=() dtype=resource>, <tf.Tensor 'unknown_227:0' shape=() dtype=resource>, <tf.Tensor 'unknown_228:0' shape=() dtype=resource>, <tf.Tensor 'unknown_229:0' shape=() dtype=resource>, <tf.Tensor 'unknown_230:0' shape=() dtype=resource>, <tf.Tensor 'unknown_231:0' shape=() dtype=resource>, <tf.Tensor 'unknown_232:0' shape=() dtype=resource>, <tf.Tensor 'unknown_233:0' shape=() dtype=resource>, <tf.Tensor 'unknown_234:0' shape=() dtype=resource>, <tf.Tensor 'unknown_235:0' shape=() dtype=resource>, <tf.Tensor 'unknown_236:0' shape=() dtype=resource>, <tf.Tensor 'unknown_237:0' shape=() dtype=resource>, <tf.Tensor 'unknown_238:0' shape=() dtype=resource>, <tf.Tensor 'unknown_239:0' shape=() dtype=resource>, <tf.Tensor 'unknown_240:0' shape=() dtype=resource>, <tf.Tensor 'unknown_241:0' shape=() dtype=resource>, <tf.Tensor 'unknown_242:0' shape=() dtype=resource>, <tf.Tensor 'unknown_243:0' shape=() dtype=resource>, <tf.Tensor 'unknown_244:0' shape=() dtype=resource>, <tf.Tensor 'unknown_245:0' shape=() dtype=resource>, <tf.Tensor 'unknown_246:0' shape=() dtype=resource>, <tf.Tensor 'unknown_247:0' shape=() dtype=resource>, <tf.Tensor 'unknown_248:0' shape=() dtype=resource>, <tf.Tensor 'unknown_249:0' shape=() dtype=resource>, <tf.Tensor 'unknown_250:0' shape=() dtype=resource>, <tf.Tensor 'unknown_251:0' shape=() dtype=resource>, <tf.Tensor 'unknown_252:0' shape=() dtype=resource>, <tf.Tensor 'unknown_253:0' shape=() dtype=resource>, <tf.Tensor 'unknown_254:0' shape=() dtype=resource>, <tf.Tensor 'unknown_255:0' shape=() dtype=resource>, <tf.Tensor 'unknown_256:0' shape=() dtype=resource>, <tf.Tensor 'unknown_257:0' shape=() dtype=resource>, <tf.Tensor 'unknown_258:0' shape=() dtype=resource>, <tf.Tensor 'unknown_259:0' shape=() dtype=resource>, <tf.Tensor 'unknown_260:0' shape=() dtype=resource>, <tf.Tensor 'unknown_261:0' shape=() dtype=resource>, <tf.Tensor 'unknown_262:0' shape=() dtype=resource>, <tf.Tensor 'unknown_263:0' shape=() dtype=resource>, <tf.Tensor 'unknown_264:0' shape=() dtype=resource>, <tf.Tensor 'unknown_265:0' shape=() dtype=resource>, <tf.Tensor 'unknown_266:0' shape=() dtype=resource>, <tf.Tensor 'unknown_267:0' shape=() dtype=resource>, <tf.Tensor 'unknown_268:0' shape=() dtype=resource>, <tf.Tensor 'unknown_269:0' shape=() dtype=resource>, <tf.Tensor 'unknown_270:0' shape=() dtype=resource>, <tf.Tensor 'unknown_271:0' shape=() dtype=resource>, <tf.Tensor 'unknown_272:0' shape=() dtype=resource>, <tf.Tensor 'unknown_273:0' shape=() dtype=resource>, <tf.Tensor 'unknown_274:0' shape=() dtype=resource>, <tf.Tensor 'unknown_275:0' shape=() dtype=resource>, <tf.Tensor 'unknown_276:0' shape=() dtype=resource>, <tf.Tensor 'unknown_277:0' shape=() dtype=resource>, <tf.Tensor 'unknown_278:0' shape=() dtype=resource>, <tf.Tensor 'unknown_279:0' shape=() dtype=resource>, <tf.Tensor 'unknown_280:0' shape=() dtype=resource>, <tf.Tensor 'unknown_281:0' shape=() dtype=resource>, <tf.Tensor 'unknown_282:0' shape=() dtype=resource>, <tf.Tensor 'unknown_283:0' shape=() dtype=resource>, <tf.Tensor 'unknown_284:0' shape=() dtype=resource>, <tf.Tensor 'unknown_285:0' shape=() dtype=resource>, <tf.Tensor 'unknown_286:0' shape=() dtype=resource>, <tf.Tensor 'unknown_287:0' shape=() dtype=resource>, <tf.Tensor 'unknown_288:0' shape=() dtype=resource>, <tf.Tensor 'unknown_289:0' shape=() dtype=resource>, <tf.Tensor 'unknown_290:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_291:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_292:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_293:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_294:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_295:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_296:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_297:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_298:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_299:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_300:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_301:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_302:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_303:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_304:0' shape=(2,) dtype=float32>, <tf.Tensor 'unknown_305:0' shape=() dtype=resource>, <tf.Tensor 'unknown_306:0' shape=() dtype=resource>, <tf.Tensor 'unknown_307:0' shape=() dtype=resource>, <tf.Tensor 'unknown_308:0' shape=() dtype=resource>, <tf.Tensor 'unknown_309:0' shape=() dtype=resource>, <tf.Tensor 'unknown_310:0' shape=() dtype=resource>, <tf.Tensor 'unknown_311:0' shape=() dtype=resource>, <tf.Tensor 'unknown_312:0' shape=() dtype=resource>, <tf.Tensor 'unknown_313:0' shape=() dtype=resource>, <tf.Tensor 'unknown_314:0' shape=() dtype=resource>, <tf.Tensor 'unknown_315:0' shape=() dtype=resource>, <tf.Tensor 'unknown_316:0' shape=() dtype=resource>, <tf.Tensor 'unknown_317:0' shape=() dtype=resource>, <tf.Tensor 'unknown_318:0' shape=() dtype=resource>, <tf.Tensor 'unknown_319:0' shape=() dtype=resource>, <tf.Tensor 'unknown_320:0' shape=() dtype=resource>, <tf.Tensor 'unknown_321:0' shape=() dtype=resource>, <tf.Tensor 'unknown_322:0' shape=() dtype=resource>, <tf.Tensor 'unknown_323:0' shape=() dtype=resource>, <tf.Tensor 'unknown_324:0' shape=() dtype=resource>, <tf.Tensor 'unknown_325:0' shape=() dtype=resource>, <tf.Tensor 'unknown_326:0' shape=() dtype=resource>, <tf.Tensor 'unknown_327:0' shape=() dtype=resource>, <tf.Tensor 'unknown_328:0' shape=() dtype=resource>, <tf.Tensor 'unknown_329:0' shape=() dtype=resource>, <tf.Tensor 'unknown_330:0' shape=() dtype=resource>, <tf.Tensor 'unknown_331:0' shape=() dtype=resource>, <tf.Tensor 'unknown_332:0' shape=() dtype=resource>, <tf.Tensor 'unknown_333:0' shape=() dtype=resource>, <tf.Tensor 'unknown_334:0' shape=() dtype=resource>, <tf.Tensor 'unknown_335:0' shape=() dtype=resource>, <tf.Tensor 'unknown_336:0' shape=() dtype=resource>, <tf.Tensor 'unknown_337:0' shape=() dtype=resource>, <tf.Tensor 'unknown_338:0' shape=() dtype=resource>, <tf.Tensor 'unknown_339:0' shape=() dtype=resource>, <tf.Tensor 'unknown_340:0' shape=() dtype=resource>, <tf.Tensor 'unknown_341:0' shape=() dtype=resource>, <tf.Tensor 'unknown_342:0' shape=() dtype=resource>, <tf.Tensor 'unknown_343:0' shape=() dtype=resource>, <tf.Tensor 'unknown_344:0' shape=() dtype=resource>, <tf.Tensor 'unknown_345:0' shape=() dtype=resource>, <tf.Tensor 'unknown_346:0' shape=() dtype=resource>, <tf.Tensor 'unknown_347:0' shape=() dtype=resource>, <tf.Tensor 'unknown_348:0' shape=() dtype=resource>, <tf.Tensor 'unknown_349:0' shape=() dtype=resource>, <tf.Tensor 'unknown_350:0' shape=() dtype=resource>, <tf.Tensor 'unknown_351:0' shape=() dtype=resource>, <tf.Tensor 'unknown_352:0' shape=() dtype=resource>, <tf.Tensor 'unknown_353:0' shape=() dtype=resource>, <tf.Tensor 'unknown_354:0' shape=() dtype=resource>, <tf.Tensor 'unknown_355:0' shape=() dtype=resource>, <tf.Tensor 'unknown_356:0' shape=() dtype=resource>, <tf.Tensor 'unknown_357:0' shape=() dtype=resource>, <tf.Tensor 'unknown_358:0' shape=() dtype=resource>, <tf.Tensor 'unknown_359:0' shape=() dtype=resource>, <tf.Tensor 'unknown_360:0' shape=() dtype=resource>, <tf.Tensor 'unknown_361:0' shape=() dtype=resource>, <tf.Tensor 'unknown_362:0' shape=() dtype=resource>, <tf.Tensor 'unknown_363:0' shape=() dtype=resource>, <tf.Tensor 'unknown_364:0' shape=() dtype=resource>, <tf.Tensor 'unknown_365:0' shape=() dtype=resource>, <tf.Tensor 'unknown_366:0' shape=() dtype=resource>, <tf.Tensor 'unknown_367:0' shape=() dtype=resource>, <tf.Tensor 'unknown_368:0' shape=() dtype=resource>, <tf.Tensor 'unknown_369:0' shape=() dtype=resource>, <tf.Tensor 'unknown_370:0' shape=() dtype=resource>, <tf.Tensor 'unknown_371:0' shape=() dtype=resource>, <tf.Tensor 'unknown_372:0' shape=() dtype=resource>, <tf.Tensor 'unknown_373:0' shape=() dtype=resource>, <tf.Tensor 'unknown_374:0' shape=() dtype=resource>, <tf.Tensor 'unknown_375:0' shape=() dtype=resource>, <tf.Tensor 'unknown_376:0' shape=() dtype=resource>, <tf.Tensor 'unknown_377:0' shape=() dtype=resource>, <tf.Tensor 'unknown_378:0' shape=() dtype=resource>, <tf.Tensor 'unknown_379:0' shape=() dtype=resource>, <tf.Tensor 'unknown_380:0' shape=() dtype=resource>, <tf.Tensor 'unknown_381:0' shape=() dtype=resource>, <tf.Tensor 'unknown_382:0' shape=() dtype=resource>, <tf.Tensor 'unknown_383:0' shape=() dtype=resource>, <tf.Tensor 'unknown_384:0' shape=() dtype=resource>, <tf.Tensor 'unknown_385:0' shape=() dtype=resource>, <tf.Tensor 'unknown_386:0' shape=() dtype=resource>, <tf.Tensor 'unknown_387:0' shape=() dtype=resource>, <tf.Tensor 'unknown_388:0' shape=() dtype=resource>, <tf.Tensor 'unknown_389:0' shape=() dtype=resource>, <tf.Tensor 'unknown_390:0' shape=() dtype=resource>, <tf.Tensor 'unknown_391:0' shape=() dtype=resource>, <tf.Tensor 'unknown_392:0' shape=() dtype=resource>, <tf.Tensor 'unknown_393:0' shape=() dtype=resource>, <tf.Tensor 'unknown_394:0' shape=() dtype=resource>, <tf.Tensor 'unknown_395:0' shape=() dtype=resource>, <tf.Tensor 'unknown_396:0' shape=() dtype=resource>, <tf.Tensor 'unknown_397:0' shape=() dtype=resource>, <tf.Tensor 'unknown_398:0' shape=() dtype=resource>, <tf.Tensor 'unknown_399:0' shape=() dtype=resource>, <tf.Tensor 'unknown_400:0' shape=() dtype=resource>, <tf.Tensor 'unknown_401:0' shape=() dtype=resource>, <tf.Tensor 'unknown_402:0' shape=() dtype=resource>, <tf.Tensor 'unknown_403:0' shape=() dtype=resource>, <tf.Tensor 'unknown_404:0' shape=() dtype=resource>, <tf.Tensor 'unknown_405:0' shape=() dtype=resource>, <tf.Tensor 'unknown_406:0' shape=() dtype=resource>, <tf.Tensor 'unknown_407:0' shape=() dtype=resource>, <tf.Tensor 'unknown_408:0' shape=() dtype=resource>, <tf.Tensor 'unknown_409:0' shape=() dtype=resource>, <tf.Tensor 'unknown_410:0' shape=() dtype=resource>, <tf.Tensor 'unknown_411:0' shape=() dtype=resource>, <tf.Tensor 'unknown_412:0' shape=() dtype=resource>, <tf.Tensor 'unknown_413:0' shape=() dtype=resource>, <tf.Tensor 'unknown_414:0' shape=() dtype=resource>, <tf.Tensor 'unknown_415:0' shape=() dtype=resource>, <tf.Tensor 'unknown_416:0' shape=() dtype=resource>, <tf.Tensor 'unknown_417:0' shape=() dtype=resource>, <tf.Tensor 'unknown_418:0' shape=() dtype=resource>, <tf.Tensor 'unknown_419:0' shape=() dtype=resource>, <tf.Tensor 'unknown_420:0' shape=() dtype=resource>, <tf.Tensor 'unknown_421:0' shape=() dtype=resource>, <tf.Tensor 'unknown_422:0' shape=() dtype=resource>, <tf.Tensor 'unknown_423:0' shape=() dtype=resource>, <tf.Tensor 'unknown_424:0' shape=() dtype=resource>, <tf.Tensor 'unknown_425:0' shape=() dtype=resource>, <tf.Tensor 'unknown_426:0' shape=() dtype=resource>, <tf.Tensor 'unknown_427:0' shape=() dtype=resource>, <tf.Tensor 'unknown_428:0' shape=() dtype=resource>, <tf.Tensor 'unknown_429:0' shape=() dtype=resource>, <tf.Tensor 'unknown_430:0' shape=() dtype=resource>, <tf.Tensor 'unknown_431:0' shape=() dtype=resource>, <tf.Tensor 'unknown_432:0' shape=() dtype=resource>, <tf.Tensor 'unknown_433:0' shape=() dtype=resource>, <tf.Tensor 'unknown_434:0' shape=() dtype=resource>, <tf.Tensor 'unknown_435:0' shape=() dtype=resource>, <tf.Tensor 'unknown_436:0' shape=() dtype=resource>, <tf.Tensor 'unknown_437:0' shape=() dtype=resource>, <tf.Tensor 'unknown_438:0' shape=() dtype=resource>, <tf.Tensor 'unknown_439:0' shape=() dtype=resource>, <tf.Tensor 'unknown_440:0' shape=() dtype=resource>, <tf.Tensor 'unknown_441:0' shape=() dtype=resource>, <tf.Tensor 'unknown_442:0' shape=() dtype=resource>, <tf.Tensor 'unknown_443:0' shape=() dtype=resource>, <tf.Tensor 'unknown_444:0' shape=() dtype=resource>, <tf.Tensor 'unknown_445:0' shape=() dtype=resource>, <tf.Tensor 'unknown_446:0' shape=() dtype=resource>, <tf.Tensor 'unknown_447:0' shape=() dtype=resource>, <tf.Tensor 'unknown_448:0' shape=() dtype=resource>, <tf.Tensor 'unknown_449:0' shape=() dtype=resource>, <tf.Tensor 'unknown_450:0' shape=() dtype=resource>, <tf.Tensor 'unknown_451:0' shape=() dtype=resource>, <tf.Tensor 'unknown_452:0' shape=() dtype=resource>, <tf.Tensor 'unknown_453:0' shape=() dtype=resource>, <tf.Tensor 'unknown_454:0' shape=() dtype=resource>, <tf.Tensor 'unknown_455:0' shape=() dtype=resource>, <tf.Tensor 'unknown_456:0' shape=() dtype=resource>, <tf.Tensor 'unknown_457:0' shape=() dtype=resource>, <tf.Tensor 'unknown_458:0' shape=() dtype=resource>, <tf.Tensor 'unknown_459:0' shape=() dtype=resource>, <tf.Tensor 'unknown_460:0' shape=() dtype=resource>, <tf.Tensor 'unknown_461:0' shape=() dtype=resource>, <tf.Tensor 'unknown_462:0' shape=() dtype=resource>, <tf.Tensor 'unknown_463:0' shape=() dtype=resource>, <tf.Tensor 'unknown_464:0' shape=() dtype=resource>, <tf.Tensor 'unknown_465:0' shape=() dtype=resource>, <tf.Tensor 'unknown_466:0' shape=() dtype=resource>, <tf.Tensor 'unknown_467:0' shape=() dtype=resource>, <tf.Tensor 'unknown_468:0' shape=() dtype=resource>, <tf.Tensor 'unknown_469:0' shape=() dtype=resource>, <tf.Tensor 'unknown_470:0' shape=() dtype=resource>, <tf.Tensor 'unknown_471:0' shape=() dtype=resource>, <tf.Tensor 'unknown_472:0' shape=() dtype=resource>, <tf.Tensor 'unknown_473:0' shape=() dtype=resource>, <tf.Tensor 'unknown_474:0' shape=() dtype=resource>, <tf.Tensor 'unknown_475:0' shape=() dtype=resource>, <tf.Tensor 'unknown_476:0' shape=() dtype=resource>]\n"
          ]
        }
      ],
      "source": [
        "print(detection_model.signatures['serving_default'].inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4rc64un_naC",
        "outputId": "92e926fe-ddc2-4d55-87a4-fd0d8d664793"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'raw_detection_boxes': tf.float32,\n",
              " 'num_detections': tf.float32,\n",
              " 'detection_boxes': tf.float32,\n",
              " 'raw_detection_scores': tf.float32,\n",
              " 'detection_multiclass_scores': tf.float32,\n",
              " 'detection_anchor_indices': tf.float32,\n",
              " 'detection_classes': tf.float32,\n",
              " 'detection_scores': tf.float32}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detection_model.signatures['serving_default'].output_dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CVpxAO8_pxu",
        "outputId": "bff142f1-5a51-4fc5-924a-b89541640bf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'raw_detection_boxes': TensorShape([1, 51150, 4]),\n",
              " 'num_detections': TensorShape([1]),\n",
              " 'detection_boxes': TensorShape([1, 100, 4]),\n",
              " 'raw_detection_scores': TensorShape([1, 51150, 2]),\n",
              " 'detection_multiclass_scores': TensorShape([1, 100, 2]),\n",
              " 'detection_anchor_indices': TensorShape([1, 100]),\n",
              " 'detection_classes': TensorShape([1, 100]),\n",
              " 'detection_scores': TensorShape([1, 100])}"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "detection_model.signatures['serving_default'].output_shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MzjWQgM_tHg"
      },
      "source": [
        "# Inferencing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtsMnH5u_sK3"
      },
      "outputs": [],
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy() \n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "   \n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])      \n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "  return output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrerzqA__w74"
      },
      "outputs": [],
      "source": [
        "def show_inference(model, image_path):\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = np.array(Image.open(image_path))\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "\n",
        "  display(Image.fromarray(image_np))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for image_path in TEST_IMAGE_PATHS:\n",
        "#  show_inference(detection_model, image_path)"
      ],
      "metadata": {
        "id": "xR4qFhovjk6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83q0xhw6_1Lu"
      },
      "outputs": [],
      "source": [
        "# My model is not predicting anything as I could not train"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}